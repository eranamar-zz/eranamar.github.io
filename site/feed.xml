<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.4.3">Jekyll</generator><link href="https://eranamar.github.io/site/feed.xml" rel="self" type="application/atom+xml" /><link href="https://eranamar.github.io/site/" rel="alternate" type="text/html" /><updated>2017-05-18T16:28:38+03:00</updated><id>https://eranamar.github.io/site/</id><title type="html">TheoryLunch Blog</title><subtitle>Computer science topics over lunch</subtitle><author><name>Eran Amar</name></author><entry><title type="html">Recurrent Neural Network - Recent Advancements</title><link href="https://eranamar.github.io/site/2017/05/18/Recurrent-Neural-Network-Recent-Advancements.html" rel="alternate" type="text/html" title="Recurrent Neural Network - Recent Advancements" /><published>2017-05-18T00:00:00+03:00</published><updated>2017-05-18T00:00:00+03:00</updated><id>https://eranamar.github.io/site/2017/05/18/Recurrent-Neural-Network---Recent-Advancements</id><content type="html" xml:base="https://eranamar.github.io/site/2017/05/18/Recurrent-Neural-Network-Recent-Advancements.html">&lt;script type=&quot;math/tex&quot;&gt;
\newcommand{\lyxlock}{}
&lt;/script&gt;

&lt;noscript&gt;
&lt;div class=&quot;warning&quot;&gt;
Warning: &lt;a href=&quot;http://www.mathjax.org/&quot;&gt;MathJax&lt;/a&gt; requires JavaScript to correctly process the mathematics on this page. Please enable JavaScript on your browser.
&lt;/div&gt;&lt;hr /&gt;
&amp;lt;/hr&amp;gt;&lt;/noscript&gt;

&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-1&quot;&gt;1&lt;/a&gt; Last Post of the Series
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
This post is the last in the series that discussed Recurrent Neural Networks (RNNs). The first post introduced the &lt;a class=&quot;URL&quot; href=&quot;https://eranamar.github.io/site/2017/04/20/Recurrent-Neural-Network-Introduction.html&quot;&gt;basic recurrent cells and their limitations&lt;/a&gt;, the second post presented the formulation of &lt;a class=&quot;URL&quot; href=&quot;https://eranamar.github.io/site/2017/04/30/Recurrent-Neural-Network-LSTM-and-GRU.html&quot;&gt;gated cells as LSTMs and GRUs&lt;/a&gt;, and the third post overviewed &lt;a class=&quot;URL&quot; href=&quot;https://eranamar.github.io/site/2017/05/12/Recurrent-Neural-Network-Dropout-for-LSTMs.html&quot;&gt;dropout variants for LSTMs&lt;/a&gt;. This post will present some of the recent advancements in the filed of RNNs (mostly related to LSTM units). 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
We start by reviewing some of the notations that we used during the series. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
We defined &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{S}_{\sigma}
&lt;/script&gt;
&lt;/span&gt; to be the family of all affine transformations &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbb{R}^{n}\times\mathbb{R}^{n}\to\mathbb{R}^{m}
&lt;/script&gt;
&lt;/span&gt; for any dimensions &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
n
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
m
&lt;/script&gt;
&lt;/span&gt;, followed by an element-wise activation function &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\sigma
&lt;/script&gt;
&lt;/span&gt;, that is,
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
&lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathcal{S}_{\sigma}=\left\{ \mathbf{x},\mathbf{h}\mapsto\sigma\left(W\mathbf{x}+U\mathbf{h}+\mathbf{b}\right)\mid\forall W,U,\mathbf{b}\right\} 

&lt;/script&gt;
&lt;/span&gt;
and noted that with the &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
sigmoid\left(x\right)=\left(1+e^{-x}\right)^{-1}
&lt;/script&gt;
&lt;/span&gt; as the activation function, all the functions in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{S}_{sig}
&lt;/script&gt;
&lt;/span&gt; are &lt;i&gt;gate&lt;/i&gt; functions, that is, their output belongs to &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left[0,1\right]^{m}
&lt;/script&gt;
&lt;/span&gt;. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Usually we do not specify &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
n
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
m
&lt;/script&gt;
&lt;/span&gt;, we assume that whenever there is a matrix multiplication of element-wise operation the dimensions always match, as they are not important for the discussion. 
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-2&quot;&gt;2&lt;/a&gt; Variations of the Gate Functions for LSTMs
&lt;/h1&gt;
&lt;h2 class=&quot;Subsection&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Subsection-2.1&quot;&gt;2.1&lt;/a&gt; Recap: LSTM Formulation - The Use of Gates
&lt;/h2&gt;
&lt;div class=&quot;Unindented&quot;&gt;
LSTM layer is a recurrent layer that keeps a memory vector &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{c}^{\text{\left(t\right)}}
&lt;/script&gt;
&lt;/span&gt; for any time step &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt;. Upon receiving the current time-step input &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{x}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt; and the output from previous time-step &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}^{\left(t-1\right)}
&lt;/script&gt;
&lt;/span&gt;, the layer computes gate activations &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
f^{\left(t\right)},i^{\left(t\right)},o^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt; by applying a the corresponding gate function &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{f},\mathbf{i},\mathbf{o}\in\mathcal{S}_{sig}
&lt;/script&gt;
&lt;/span&gt; on the tuple &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)
&lt;/script&gt;
&lt;/span&gt;, note that those activations are vectors in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left[0,1\right]^{m}
&lt;/script&gt;
&lt;/span&gt;. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
A new memory-vector is then being computed by &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{c}^{\left(t\right)}=f^{\left(t\right)}\circ\mathbf{c}^{\left(t-1\right)}+i^{\left(t\right)}\circ\mathbf{s}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)

&lt;/script&gt;
&lt;/span&gt;
 where &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\circ
&lt;/script&gt;
&lt;/span&gt; is element-wise multiplication and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{s}\in\mathcal{S}_{tanh}
&lt;/script&gt;
&lt;/span&gt; is the state-transition function. The layer then outputs &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}^{\left(t\right)}=o^{\left(t\right)}\circ tanh\left(\mathbf{c}^{\left(t\right)}\right)
&lt;/script&gt;
&lt;/span&gt;. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
The gate functions are of great importance for LSTM layer, they are what allows the layer to learn short and long term dependencies, and they also help avoiding the &lt;i&gt;exploding and vanishing gradients problem&lt;/i&gt;.
&lt;/div&gt;
&lt;h2 class=&quot;Subsection&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Subsection-2.2&quot;&gt;2.2&lt;/a&gt; Simple Variations
&lt;/h2&gt;
&lt;div class=&quot;Unindented&quot;&gt;
The more expressive power a gate function have, the better it can learn short \ long term dependencies. That is, if a gate function is very rich, it can account for subtle changes in the input &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{x}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt; as well to subtle changes in the “past”, i.e. in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}^{\left(t-1\right)}
&lt;/script&gt;
&lt;/span&gt;. One way to enrich a gate function is by making it depends on the previous memory vector &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{c}^{\left(t-1\right)}
&lt;/script&gt;
&lt;/span&gt; in addition to the regular tuple &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)
&lt;/script&gt;
&lt;/span&gt;. That will redefine our gate functions family to be &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathcal{S}_{\sigma}=\left\{ \mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)},\mathbf{c}^{\left(t-1\right)}\mapsto\sigma\left(W\mathbf{x}^{\left(t\right)}+U\mathbf{h}^{\left(t-1\right)}+V\mathbf{c}^{\left(t-1\right)}+\mathbf{b}\right)\right\} 

&lt;/script&gt;
&lt;/span&gt;
for any &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
W,U
&lt;/script&gt;
&lt;/span&gt; and diagonal &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
V
&lt;/script&gt;
&lt;/span&gt;. Such construction can be found in &lt;a class=&quot;URL&quot; href=&quot;https://arxiv.org/abs/1308.0850&quot;&gt;here&lt;/a&gt;, note that we can think of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
V\mathbf{c}^{\left(t-1\right)}
&lt;/script&gt;
&lt;/span&gt; as a bias vector that is memory-dependent, thus there are formulation in which the vector &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{b}
&lt;/script&gt;
&lt;/span&gt; is omitted. 
&lt;/div&gt;
&lt;h2 class=&quot;Subsection&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Subsection-2.3&quot;&gt;2.3&lt;/a&gt; Multiplicative LSTMs
&lt;/h2&gt;
&lt;div class=&quot;Unindented&quot;&gt;
In &lt;a class=&quot;URL&quot; href=&quot;https://arxiv.org/abs/1609.07959&quot;&gt;this paper&lt;/a&gt; from October 2016, the authors took that idea one step further. They wanted to make a unique gate function for each possible input. Recall that any function in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{S}_{\sigma}
&lt;/script&gt;
&lt;/span&gt; is of the form &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\mapsto W\mathbf{x}^{\left(t\right)}+U\mathbf{h}^{\left(t-1\right)}+\mathbf{b}
&lt;/script&gt;
&lt;/span&gt;. For simplicity, consider the case where &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{b}=\mathbf{0}
&lt;/script&gt;
&lt;/span&gt;. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
We ends up with a sum of two components: one that depends on the current input and one that depends on the past (i.e. &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}^{\left(t-1\right)}
&lt;/script&gt;
&lt;/span&gt; which encodes information about previous time steps), and the component with larger magnitude will dominate the transition. If &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
W\mathbf{x}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt; is larger, the layer will not be sensitive enough to the past, and if &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
U\mathbf{h}^{\left(t-1\right)}
&lt;/script&gt;
&lt;/span&gt; is larger, then the layer will not be sensitive to subtle changes in the input.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
The author noted that since in most cases the input &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{x}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt; is an 1-hot vector, then multiply by &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
W
&lt;/script&gt;
&lt;/span&gt; is just selecting a specific column. So we may think of any gate function &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{s}\in\mathcal{S}_{\sigma}
&lt;/script&gt;
&lt;/span&gt; as a &lt;b&gt;fixed&lt;/b&gt; base affine transformation &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}\mapsto U\mathbf{h}
&lt;/script&gt;
&lt;/span&gt; combined with input-dependent bias vector &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{b}_{\mathbf{x}^{\left(t\right)}}=W\mathbf{x}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt;. That is &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{s}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)=U\mathbf{h}^{\left(t-1\right)}+\mathbf{b}_{\mathbf{x}^{\left(t\right)}}

&lt;/script&gt;
&lt;/span&gt;
that formula emphasis the &lt;i&gt;additive&lt;/i&gt; effect of the current input-vector on the transition of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}^{\left(t-1\right)}
&lt;/script&gt;
&lt;/span&gt;. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
The goal in multiplicative LSTM (mLSTM), is to have an &lt;b&gt;unique&lt;/b&gt; affine transformation (in our case, unique matrix &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
U
&lt;/script&gt;
&lt;/span&gt;) for each possible input. Obviously, if the number of possible inputs is very large, the number of the parameters will explode and it won’t be feasible to train the network. To overcome that, the authors of the paper suggested to learn &lt;i&gt;shared&lt;/i&gt; intermediate matrices that will be used to construct an unique &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
U
&lt;/script&gt;
&lt;/span&gt; for each input, and because the factorization is shared, there are less parameters to learn. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
The factorization is defined as follow: the unique matrix &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
U_{\mathbf{x}^{\left(t\right)}}
&lt;/script&gt;
&lt;/span&gt; is constructed by &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
V_{1}diag\left(T\mathbf{x}^{\left(t\right)}\right)V_{2}
&lt;/script&gt;
&lt;/span&gt; where &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
V_{1},V_{2}
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
T
&lt;/script&gt;
&lt;/span&gt; are intermediate matrices which are shared across all the possible inputs, and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
diag\left(\mathbf{v}\right)
&lt;/script&gt;
&lt;/span&gt; is an operation that maps any vector &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{v}
&lt;/script&gt;
&lt;/span&gt; to a square diagonal matrix with the elements of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{v}
&lt;/script&gt;
&lt;/span&gt; on its diagonal.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Note that the target dimension of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
T
&lt;/script&gt;
&lt;/span&gt; can be set to be arbitrary large (in the paper they chose it to be the same as &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt;).
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
The difference between LSTM and mLSTM is only in the definition of the gate functions family, all the equations for updating the memory cell and the output are the same. In mLSTM the family &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{S}_{\sigma}
&lt;/script&gt;
&lt;/span&gt; is being replaced with &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;
\begin{aligned}
\mathcal{S}'_{\sigma} &amp; =\left\{ \mathbf{x},\mathbf{h}\mapsto\sigma\left(W\mathbf{x}+U_{\mathbf{x}}\mathbf{h}\right)\right\} \\
 &amp; =\left\{ \mathbf{x},\mathbf{h}\mapsto\sigma\left(\mathbf{b}_{\mathbf{x}}+V_{1}diag\left(T\mathbf{x}\right)V_{2}\mathbf{h}\right)\mid\forall W,V_{1},V_{2},T\right\} 
\end{aligned}
&lt;/script&gt;
&lt;/span&gt;
Note that we can reduce the number of parameters even further by forcing all the gate functions to use the same &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
V_{2}
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
T
&lt;/script&gt;
&lt;/span&gt; matrices. That is, each gate will be parametrized only by &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
W
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
V_{1}
&lt;/script&gt;
&lt;/span&gt;. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Formally, define the following transformation &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\tau:\mathbb{R}^{n}\times\mathbb{R}^{n}\to\mathbb{R}^{m}
&lt;/script&gt;
&lt;/span&gt; such that &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\tau\left(\mathbf{x},\mathbf{h}\right)=T\mathbf{x}\circ V_{2}\mathbf{h}
&lt;/script&gt;
&lt;/span&gt; and then we can define &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{S}'_{\sigma}
&lt;/script&gt;
&lt;/span&gt; for some fixed learned transformation &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\tau
&lt;/script&gt;
&lt;/span&gt; as &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;
\begin{aligned}
\mathcal{S}'_{\sigma,\tau} &amp; =\left\{ \mathbf{x},\mathbf{h}\mapsto\sigma\left(W\mathbf{x}+V_{1}\tau\left(\mathbf{x},\mathbf{h}\right)\right)\mid\forall W,V_{1}\right\} \\
 &amp; =\left\{ \mathbf{x},\mathbf{h}\mapsto\mathbf{s}\left(\mathbf{x},\tau\left(\mathbf{h}\right)\right)\mid\mathbf{s}\in\mathcal{S}_{\sigma}\right\} 
\end{aligned}
&lt;/script&gt;
&lt;/span&gt;
 in other words, mLSTM is an LSTM that apply its gates to the tuple &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left(\mathbf{x}^{\left(t\right)},\tau\left(\mathbf{h}^{\left(t-1\right)}\right)\right)
&lt;/script&gt;
&lt;/span&gt; rather than &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)
&lt;/script&gt;
&lt;/span&gt;, and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\tau
&lt;/script&gt;
&lt;/span&gt; is another learned transformation. If you look again at the exact formula of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\tau
&lt;/script&gt;
&lt;/span&gt; to will see the &lt;i&gt;multiplicative&lt;/i&gt; effect of the input vector on the transformation of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}
&lt;/script&gt;
&lt;/span&gt;. That way, the author said, &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{S}'_{\sigma,\tau}
&lt;/script&gt;
&lt;/span&gt; can yield much richer family of input-dependent transformations for &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt;, which can be sensitive to the past as well as to subtle changes in the current input.
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-3&quot;&gt;3&lt;/a&gt; Recurrent Batch Normalization
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
Batch normalization is an operator applied to a layer before going through the non-linearity function in order to “normalize” the values before activation. That operator has two hyper-parameters for tuning and two statistics that it accumulates internally. Formally, given a vector of pre-activation values &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{x}
&lt;/script&gt;
&lt;/span&gt;, batch-normalization is &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

BN\left(\mathbf{x};\gamma,\beta\right)=\beta+\gamma\circ\frac{\mathbf{x}-\mathbb{\hat{E}\left[\mathbf{x}\right]}}{\sqrt{\hat{\mathbb{V}}\left[\mathbf{x}\right]+\epsilon}}

&lt;/script&gt;
&lt;/span&gt;
where &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\hat{\mathbb{E}}
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\hat{\mathbb{V}}
&lt;/script&gt;
&lt;/span&gt; are the empirical mean and variance of the current batch respectively. &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\gamma,\beta
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\epsilon
&lt;/script&gt;
&lt;/span&gt; are vectors, and the division in the operator is being computed element-wise. Note that at inference time, the population statistics are estimated by averaging the empirical statistics across all the batches.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
&lt;a class=&quot;URL&quot; href=&quot;https://arxiv.org/abs/1603.09025&quot;&gt;That paper&lt;/a&gt; from February 2017, applied batch normalization also to the &lt;i&gt;recurrent connections&lt;/i&gt; in LSTM layers, and they showed empirically that it helped the network to converge faster. The usage is simple, each gate function &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{i},\mathbf{f},\mathbf{o}\in\mathcal{S}_{sig}
&lt;/script&gt;
&lt;/span&gt; and the transition function &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{s}\in\mathcal{S}_{tanh}
&lt;/script&gt;
&lt;/span&gt; computes &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\mapsto\sigma\left(BN\left(W\mathbf{h}^{\left(t-1\right)};\gamma_{h},\beta_{h}\right)+BN\left(W\mathbf{x}^{\left(t\right)};\gamma_{x},\beta_{x}\right)+\mathbf{b}\right)

&lt;/script&gt;
&lt;/span&gt;
 when the hyper-parameters &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\gamma
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\beta
&lt;/script&gt;
&lt;/span&gt; are shared across different gates. Then the output of the layer is &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}^{\left(t\right)}=o^{\left(t\right)}\circ tanh\left(BN\left(\mathbf{c}^{\left(t\right)};\gamma_{c},\beta_{c}\right)\right)
&lt;/script&gt;
&lt;/span&gt;. The authors suggested to set &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\beta_{x}=\beta_{h}=\mathbf{0}
&lt;/script&gt;
&lt;/span&gt; because there is already a bias parameter &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{b}
&lt;/script&gt;
&lt;/span&gt; and to prevent redundancy. In addition, they said that sharing the internal &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
BN
&lt;/script&gt;
&lt;/span&gt; statistics across time degrades performance severely. Therefore, one should use “fresh” &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
BN
&lt;/script&gt;
&lt;/span&gt; operators for each time-step with their own internal statistics (but share the &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\beta
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\gamma
&lt;/script&gt;
&lt;/span&gt; parameters).
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-4&quot;&gt;4&lt;/a&gt; The Evergrowing Field
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
RNNs is a rapidly growing field and this series only covered a small part of it. There are much more advance models, some of them are only from few months ago. If you are interested in this field, you may find very interesting stuff &lt;a class=&quot;URL&quot; href=&quot;https://smerity.com/articles/2016/iclr_2017_submissions.html&quot;&gt;here&lt;/a&gt;. 
&lt;/div&gt;</content><author><name>Eran Amar</name></author><category term="neural_networks" /><summary type="html">Warning: MathJax requires JavaScript to correctly process the mathematics on this page. Please enable JavaScript on your browser. &amp;lt;/hr&amp;gt;</summary></entry><entry><title type="html">Recurrent Neural Network - Dropout for LSTMs</title><link href="https://eranamar.github.io/site/2017/05/12/Recurrent-Neural-Network-Dropout-for-LSTMs.html" rel="alternate" type="text/html" title="Recurrent Neural Network - Dropout for LSTMs" /><published>2017-05-12T00:00:00+03:00</published><updated>2017-05-12T00:00:00+03:00</updated><id>https://eranamar.github.io/site/2017/05/12/Recurrent-Neural-Network---Dropout-for-LSTMs</id><content type="html" xml:base="https://eranamar.github.io/site/2017/05/12/Recurrent-Neural-Network-Dropout-for-LSTMs.html">&lt;script type=&quot;math/tex&quot;&gt;
\newcommand{\lyxlock}{}
&lt;/script&gt;

&lt;noscript&gt;
&lt;div class=&quot;warning&quot;&gt;
Warning: &lt;a href=&quot;http://www.mathjax.org/&quot;&gt;MathJax&lt;/a&gt; requires JavaScript to correctly process the mathematics on this page. Please enable JavaScript on your browser.
&lt;/div&gt;&lt;hr /&gt;
&amp;lt;/hr&amp;gt;&lt;/noscript&gt;

&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-1&quot;&gt;1&lt;/a&gt; Recap
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
This is the third post in the series about Recurrent Neural Networks. The &lt;a class=&quot;URL&quot; href=&quot;https://eranamar.github.io/site/2017/04/20/Recurrent-Neural-Network-Introduction.html&quot;&gt;first post &lt;/a&gt;defined the basic notations and the formulation of a Recurrent Cell, the &lt;a class=&quot;URL&quot; href=&quot;https://eranamar.github.io/site/2017/04/30/Recurrent-Neural-Network-LSTM-and-GRU.html&quot;&gt;second post&lt;/a&gt; discussed its extensions such as LSTM and GRU. Recall that LSTM layer receives vector &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{x}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt; - the input for the current time step &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt;, and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{c}^{\left(t-1\right)},\mathbf{h}^{\left(t-1\right)}
&lt;/script&gt;
&lt;/span&gt; the memory-vector and the output-vector from previous time-step respectively, then the layer computes a new memory vector &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{c}^{\left(t\right)}=\mathbf{f}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)\circ\mathbf{c}^{\left(t-1\right)}+\mathbf{i}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)\circ\mathbf{s}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)

&lt;/script&gt;
&lt;/span&gt;
where &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\circ
&lt;/script&gt;
&lt;/span&gt; denotes element wise multiplication. Then the layer outputs &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{h}^{\left(t\right)}=\mathbf{o}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)\circ tanh\left(\mathbf{c}^{\left(t\right)}\right)

&lt;/script&gt;
&lt;/span&gt;
where &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{f},\mathbf{i},\mathbf{o}\in\mathcal{S}_{sigmoid}
&lt;/script&gt;
&lt;/span&gt; are the &lt;i&gt;forget&lt;/i&gt;, &lt;i&gt;input&lt;/i&gt; and &lt;i&gt;output&lt;/i&gt; gates respectively, and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{s}\in\mathcal{S}_{tanh}
&lt;/script&gt;
&lt;/span&gt; is the state transition function. &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{S}_{\sigma}
&lt;/script&gt;
&lt;/span&gt; denotes a family of all affine transformations &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbb{R}^{n}\times\mathbb{R}^{n}\to\mathbb{R}^{m}
&lt;/script&gt;
&lt;/span&gt; following by an element-wise activation function &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\sigma
&lt;/script&gt;
&lt;/span&gt;, for any input-dimension &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
n
&lt;/script&gt;
&lt;/span&gt; and output-dimension &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
m
&lt;/script&gt;
&lt;/span&gt;.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
In this post, we will focus on the different variants of dropout regularization that are being used in LSTM networks. 
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-2&quot;&gt;2&lt;/a&gt; Dropout Regularization
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
Dropout is a very popular regularization mechanism that is being attached to layers of a neural network in order to reduce overfitting and improve generalization. Applying dropout to a layer starts by fixing some &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
p\in\left(0,1\right)
&lt;/script&gt;
&lt;/span&gt;. Then, any time that layer produces an output during training, each one of its neurons is being zeroed-out independently with probability &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
p
&lt;/script&gt;
&lt;/span&gt;, and with probability &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
1-p
&lt;/script&gt;
&lt;/span&gt; it is being scaled by &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\frac{1}{1-p}
&lt;/script&gt;
&lt;/span&gt;. Scaling the output is important because it keeps the expected output-value of the neuron unchanged. During test time, the dropout mechanism is being turned-off, that is, the output of each neuron is being passed as is. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
For fully-connected layers, we can formulate dropout as follows: Suppose that &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}
&lt;/script&gt;
&lt;/span&gt; should be the output of some layer that has dropout mechanism. Generate a mask &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{z}\in\left\{ 0,1\right\} ^{n}
&lt;/script&gt;
&lt;/span&gt;, by picking each coordinate i.i.d w.p. &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
p
&lt;/script&gt;
&lt;/span&gt; from &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left\{ 0,1\right\} 
&lt;/script&gt;
&lt;/span&gt;, and output &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}\circ\mathbf{z}
&lt;/script&gt;
&lt;/span&gt; instead of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}
&lt;/script&gt;
&lt;/span&gt;. Note that we hide here the output-scaling for simplicity, and that the mask &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{z}
&lt;/script&gt;
&lt;/span&gt; is being generated any time the layer required to produce an output. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
At first glance it seems trivial to apply dropout to LSTM layers. However, when we come to implement the mechanism we encounter some technical issues that should be addressed. For instance, we need to decide whether to apply the given mask only to the input &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{x}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt; or also to the recurrent connection (i.e. to &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}^{\left(t-1\right)}
&lt;/script&gt;
&lt;/span&gt;), and if we choose to apply on both, should it be the same mask or different masks? Should a mask be shared &lt;i&gt;across time&lt;/i&gt; or be generated for each time-step?
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Several works tried to apply dropout to LSTM layers in several naive ways but without success. It seems that just dropping randomly some coordinates from the recurrent connections &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}^{\left(t-1\right)}
&lt;/script&gt;
&lt;/span&gt; impair the ability of the LSTM layer to learn long\short term dependencies. In the next section we will review some works that applied dropout to LSTMs in a way that successfully yields better generalization. 
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-3&quot;&gt;3&lt;/a&gt; Variants
&lt;/h1&gt;
&lt;h2 class=&quot;Subsection&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Subsection-3.1&quot;&gt;3.1&lt;/a&gt; Mask Only Inputs; Regenerate Masks
&lt;/h2&gt;
&lt;div class=&quot;Unindented&quot;&gt;
Wojciech Zaremba, Ilya Sutskever and Oriol Vinyals published in 2014 a &lt;a class=&quot;URL&quot; href=&quot;https://arxiv.org/abs/1409.2329v5&quot;&gt;paper&lt;/a&gt; that describe a successful dropout variation for LSTM layers. Their idea was that dropout should be applied only to the inputs of the layer and not to the recurrent connections. Moreover, a new mask should be generated for each time step. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Formally, for each time-step &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt;, generate a mask &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{z}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt; and compute &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{\hat{x}}^{\left(t\right)}:=\mathbf{x}^{\left(t\right)}\circ\mathbf{z}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt;. Then continue to compute the LSTM layer as usual but use &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{\hat{x}}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt; as the input to the layer rather than &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{x}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt;. 
&lt;/div&gt;
&lt;h2 class=&quot;Subsection&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Subsection-3.2&quot;&gt;3.2&lt;/a&gt; rnnDrop: Mask Only the Memory; Fixed Mask
&lt;/h2&gt;
&lt;div class=&quot;Unindented&quot;&gt;
On 2015, Taesup Moon, Heeyoul Choi, Hoshik Lee and Inchul Song &lt;a class=&quot;URL&quot; href=&quot;https://www.stat.berkeley.edu/~tsmoon/files/Conference/asru2015.pdf&quot;&gt;published&lt;/a&gt; a different dropout variation: &lt;i&gt;rnnDrop&lt;/i&gt;. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
They suggested to generate a mask for each training sequence and fix it for all the time-steps in that sequence, that is, the mask is being &lt;i&gt;shared across time&lt;/i&gt;. The mask is then being applied to the &lt;i&gt;memory vector&lt;/i&gt; of the layer rather than the input. In their formulation, only the second formula of the LSTM changed: &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{h}^{\left(t\right)}=\mathbf{o}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)\circ tanh\left(\mathbf{c}^{\left(t\right)}\circ\mathbf{z}\right)

&lt;/script&gt;
&lt;/span&gt;
where &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{z}
&lt;/script&gt;
&lt;/span&gt; if the fixed mask to the entire current training sequence.
&lt;/div&gt;
&lt;h2 class=&quot;Subsection&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Subsection-3.3&quot;&gt;3.3&lt;/a&gt; Mask Input and Hidden; Fixed Mask
&lt;/h2&gt;
&lt;div class=&quot;Unindented&quot;&gt;
A relatively &lt;a class=&quot;URL&quot; href=&quot;https://arxiv.org/abs/1512.05287&quot;&gt;recent work&lt;/a&gt; of Yarin Gal and Zoubin Ghahramani from 2016 also use a make that is shared across time, however it is being applied to the inputs as well on the &lt;i&gt;recurrent connections&lt;/i&gt;. This is one of the first successful dropout variants that actually apply the mask to the recurrent connection. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Formally, for each training sequence generate two masks &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{z}_{1},\mathbf{z}_{2}
&lt;/script&gt;
&lt;/span&gt; and compute &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{\hat{x}}^{\left(t\right)}:=\mathbf{x}^{\left(t\right)}\circ\mathbf{z}_{1}
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{\hat{h}}^{\left(t-1\right)}:=\mathbf{h}^{\left(t-1\right)}\circ\mathbf{z}_{2}
&lt;/script&gt;
&lt;/span&gt;. Then use &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{\hat{x}}^{\left(t\right)},\mathbf{\hat{h}}^{\left(t-1\right)}
&lt;/script&gt;
&lt;/span&gt; as the input to the “regular” LSTM layer.
&lt;/div&gt;
&lt;h2 class=&quot;Subsection&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Subsection-3.4&quot;&gt;3.4&lt;/a&gt; Mask Gates; Fixed Mask
&lt;/h2&gt;
&lt;div class=&quot;Unindented&quot;&gt;
Another &lt;a class=&quot;URL&quot; href=&quot;https://arxiv.org/abs/1603.05118&quot;&gt;paper&lt;/a&gt; from 2016, of Stanislau Semeniuta, Aliaksei Severyn and Erhardt Barth demonstrate the mask being applied to some of the &lt;i&gt;gates&lt;/i&gt; rather than the input\hidden vectors. For any time-step, generate &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{z}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt; and then use it to mask the &lt;i&gt;input gate&lt;/i&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{c}^{\left(t\right)}=\mathbf{f}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)\circ\mathbf{c}^{\left(t-1\right)}+\mathbf{z}^{\left(t\right)}\circ\mathbf{i}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)\circ\mathbf{s}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)

&lt;/script&gt;
&lt;/span&gt;
and the second LSTM equation left unchanged. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Small note, the authors also addressed in their paper some issues related to scaling the not-dropped coordinates, which won’t be covered here.
&lt;/div&gt;
&lt;h2 class=&quot;Subsection&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Subsection-3.5&quot;&gt;3.5&lt;/a&gt; Zoneout
&lt;/h2&gt;
&lt;div class=&quot;Unindented&quot;&gt;
The very last dropout variation is by &lt;a class=&quot;URL&quot; href=&quot;https://arxiv.org/abs/1606.01305&quot;&gt;David Krueger et al&lt;/a&gt;, from 2017. They suggested to treat the memory vector &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{c}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt; and the output vector (a.k.a &lt;i&gt;hidden&lt;/i&gt; vector) &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt; as follows: each coordinate in each of the vectors either being updated as usual or &lt;i&gt;preserved its value&lt;/i&gt; from previous time-step. As opposed to regular dropout, where “dropping a coordinate” meaning to make it zero, in zoneout it just keeps its previous value - acting as a random identity map that allows better gradients propagation through more time steps. Note that preserving a coordinate in the memory vector should not affect the computation of the hidden vector. Therefore we have to rewrite the formulas for the LSTM layer: given &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{c}^{\left(t-1\right)}
&lt;/script&gt;
&lt;/span&gt;. Generate two masks &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{z}_{1}^{\left(t\right)},\mathbf{z}_{2}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt; for the current time-step &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt;. Start by computing a candidate memory-vector with the regular formula, &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{\hat{c}}^{\left(t\right)}=\mathbf{f}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)\circ\mathbf{c}^{\left(t-1\right)}+\mathbf{i}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)\circ\mathbf{s}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)

&lt;/script&gt;
&lt;/span&gt;
and then use only &lt;i&gt;some&lt;/i&gt; of its coordinates to update the memory-vector, that is, &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{c}^{\left(t\right)}=\mathbf{z}_{1}^{\left(t\right)}\circ\mathbf{c}^{\left(t-1\right)}+\left(1-\mathbf{z}_{1}^{\left(t\right)}\right)\circ\mathbf{\hat{c}}^{\left(t\right)}

&lt;/script&gt;
&lt;/span&gt;
Similarly, for the hidden-vector, compute a candidate &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{\hat{h}}^{\left(t\right)}=\mathbf{o}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)\circ tanh\left(\mathbf{\hat{c}}^{\left(t\right)}\right)

&lt;/script&gt;
&lt;/span&gt;
and selectively update &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{h}^{\left(t\right)}=\mathbf{z}_{2}^{\left(t\right)}\circ\mathbf{h}^{\left(t-1\right)}+\left(1-\mathbf{z}_{2}^{\left(t\right)}\right)\circ\mathbf{\hat{h}}^{\left(t\right)}

&lt;/script&gt;
&lt;/span&gt;
Observe again that the computation of the candidate hidden-vector is based on &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{\hat{c}}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt; therefore unaffected by the mask &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{z}_{1}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt;.
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-4&quot;&gt;4&lt;/a&gt; More and More and More
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
In the last year, dropout for LSTM networks gained more attention, and the list of different variants is growing quickly. This post tried to cover only &lt;i&gt;some&lt;/i&gt; fraction of all the variants available out there. It is left for the interested reader to search for the literature for more about this topic. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Note that some of the dropout variations discussed above can be applied to basic RNN and GRU cells without much modifications - we refer you to the papers themselves for more details. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Next post, will discuss recent advancement in the field of RNNs, such as Multiplicative LSTMs, Recurrent Batch Normalization and more.
&lt;/div&gt;</content><author><name>Eran Amar</name></author><category term="neural_networks" /><summary type="html">Warning: MathJax requires JavaScript to correctly process the mathematics on this page. Please enable JavaScript on your browser. &amp;lt;/hr&amp;gt;</summary></entry><entry><title type="html">Recurrent Neural Network - LSTM and GRU</title><link href="https://eranamar.github.io/site/2017/04/30/Recurrent-Neural-Network-LSTM-and-GRU.html" rel="alternate" type="text/html" title="Recurrent Neural Network - LSTM and GRU" /><published>2017-04-30T00:00:00+03:00</published><updated>2017-04-30T00:00:00+03:00</updated><id>https://eranamar.github.io/site/2017/04/30/Recurrent-Neural-Network---LSTM-and-GRU</id><content type="html" xml:base="https://eranamar.github.io/site/2017/04/30/Recurrent-Neural-Network-LSTM-and-GRU.html">&lt;script type=&quot;math/tex&quot;&gt;
\newcommand{\lyxlock}{}
&lt;/script&gt;

&lt;noscript&gt;
&lt;div class=&quot;warning&quot;&gt;
Warning: &lt;a href=&quot;http://www.mathjax.org/&quot;&gt;MathJax&lt;/a&gt; requires JavaScript to correctly process the mathematics on this page. Please enable JavaScript on your browser.
&lt;/div&gt;&lt;hr /&gt;
&amp;lt;/hr&amp;gt;&lt;/noscript&gt;

&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-1&quot;&gt;1&lt;/a&gt; Recap
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
The &lt;a class=&quot;URL&quot; href=&quot;https://eranamar.github.io/site/2017/04/20/Recurrent-Neural-Network-Introduction.html&quot;&gt;first post&lt;/a&gt; in the series discussed the basic structure of recurrent cells and their limitations. We defined two families of functions, the first is &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{F}_{\sigma}
&lt;/script&gt;
&lt;/span&gt; which contains all the affine transformations &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbb{R}^{n}\to\mathbb{R}^{m}
&lt;/script&gt;
&lt;/span&gt; for any &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
n
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
m,
&lt;/script&gt;
&lt;/span&gt; followed by an element-wise activation function &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\sigma.
&lt;/script&gt;
&lt;/span&gt; And another family &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{S}_{\sigma}
&lt;/script&gt;
&lt;/span&gt; which is some kind of extension of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{F}_{\sigma}
&lt;/script&gt;
&lt;/span&gt; in the sense that the input space is &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbb{R}^{n}\times\mathbb{R}^{n}
&lt;/script&gt;
&lt;/span&gt; rather than &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbb{R}^{n}.
&lt;/script&gt;
&lt;/span&gt; Formally, the definition of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{S}_{\sigma}
&lt;/script&gt;
&lt;/span&gt; is&lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathcal{S}_{\sigma}=\left\{ \mathbf{x},\mathbf{h}\mapsto\sigma\left(W\mathbf{x}+U\mathbf{h}+\mathbf{b}\right)\mid\forall W,U,\mathbf{b}\right\} 

&lt;/script&gt;
&lt;/span&gt;
In this post, the second in the series,  we will focus on two activation functions: &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
tanh\left(x\right)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
sig\left(x\right)=\left(1+e^{-x}\right)^{-1}
&lt;/script&gt;
&lt;/span&gt;. Next section describe what are &lt;i&gt;gate&lt;/i&gt; functions, then we move to review two common improvements to the basic recurrent cell: the LSTM and GRU cells. 
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-2&quot;&gt;2&lt;/a&gt; Gate Functions
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
Recall that recurrent layer has two steps. The first is updating its inner state based on both the current input and the previous state vector, then producing an output by applying some other function to the new state. So the &lt;b&gt;input&lt;/b&gt; to the layer at each time step &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt; is actually the tuple &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)
&lt;/script&gt;
&lt;/span&gt;.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
A &lt;i&gt;gate&lt;/i&gt; is a function that takes such a tuple and produces a vector of values between zero and one. Note that any function in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{S}_{sig}
&lt;/script&gt;
&lt;/span&gt; and in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{S}_{tanh}
&lt;/script&gt;
&lt;/span&gt; is actually a gate because of the properties of the &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
tanh
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
sig
&lt;/script&gt;
&lt;/span&gt; functions. We add gates into a recurrent neuron in order to control how much information will flow on the recurrent connection. That is, suppose that &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt; is the layer’s current computed state that should be used on the next time step, a gated layer will use the vector &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\hat{\mathbf{h}}^{\left(t\right)}=\mathbf{h}^{\left(t\right)}\circ\mathbf{s}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right),
&lt;/script&gt;
&lt;/span&gt; instead of using &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt; (when the symbol &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\circ
&lt;/script&gt;
&lt;/span&gt; denotes element-wise multiplication). Note that any coordinate in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\hat{\mathbf{h}}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt; is a “moderated” version of the corresponding coordinate in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt; because each entry in the gate’s output is in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left(0,1\right).
&lt;/script&gt;
&lt;/span&gt; We will see a concrete example in the next section. 
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-3&quot;&gt;3&lt;/a&gt; LSTM Cell
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
Long Short Term Memory Cell (LSTM cell) is an improved version of the recurrent neuron that was proposed on 1997. It went through minor modifications until the version presented below (which is from 2013). LSTM cells solve both limitations of the basic recurrent neuron; it prevents the exploding and vanishing gradients problem, and it can &lt;i&gt;remember&lt;/i&gt; as well to &lt;i&gt;forget&lt;/i&gt;. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
The main addition to the recurrent layer structure is the use of gates and a memory vector for each time step, denoted by &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{c}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt;. The LSTM layer gets as inputs the tuple &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)
&lt;/script&gt;
&lt;/span&gt; and the previous memory vector &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{c}^{\left(t-1\right)},
&lt;/script&gt;
&lt;/span&gt; then outputs &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt; and an updated memory vector &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{c}^{\left(t\right)}.
&lt;/script&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Here is how an LSTM layer computes its outputs: it have the following gates &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{f},\mathbf{i},\mathbf{o}\in\mathcal{S}_{sig}
&lt;/script&gt;
&lt;/span&gt; named the &lt;i&gt;forget&lt;/i&gt;, &lt;i&gt;input&lt;/i&gt; and &lt;i&gt;output&lt;/i&gt; gate respectively, and a state-transition function &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{s}\in\mathcal{S}_{tanh}
&lt;/script&gt;
&lt;/span&gt;. It first updates the memory vector &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{c}^{\left(t\right)}=\mathbf{f}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)\circ\mathbf{c}^{\left(t-1\right)}+\mathbf{i}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)\circ\mathbf{s}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)

&lt;/script&gt;
&lt;/span&gt;
and then computes&lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{h}^{\left(t\right)}=\mathbf{o}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)\circ tanh\left(\mathbf{c}^{\left(t\right)}\right)

&lt;/script&gt;
&lt;/span&gt;
 Those equations can be explained as follows: the first equation is an element-wise summation of two terms. The first term is the previous memory vector &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{c}^{\left(t-1\right)}
&lt;/script&gt;
&lt;/span&gt; moderated by the &lt;i&gt;forget gate&lt;/i&gt;. That is, the layer uses the current input &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{x}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt; and previous output &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}^{\left(t-1\right)}
&lt;/script&gt;
&lt;/span&gt; to determine how much to shrink each coordinate of the previous memory vector. The second term is the candidate for the new state, i.e. &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{s}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right),
&lt;/script&gt;
&lt;/span&gt; moderated by the &lt;i&gt;input gate&lt;/i&gt;. Note that all the gates operate on the same input tuple &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right).
&lt;/script&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
The &lt;i&gt;input&lt;/i&gt; and &lt;i&gt;forget&lt;/i&gt; gates control the long-short term dependencies (i.e. the recurrent connection), and allow the LSTM layer to adaptively control the balance between new information that come from the state-transition function and the history information that comes from the memory vector, hence the names for the gates: &lt;i&gt;input&lt;/i&gt; and &lt;i&gt;forget&lt;/i&gt;.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Another difference from the basic recurrent layer is that LSTM layer controls how much of its inner-memory to expose by using the &lt;i&gt;output&lt;/i&gt; gate. That is being formulated in the second equation. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
The addition of gates is what preventing the exploding and vanishing gradients problem. It makes the LSTM layer able to learn long and short term dependencies at the cost of increasing the number of parameters that are needed to be trained, and that makes the network harder to optimized.
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-4&quot;&gt;4&lt;/a&gt; GRU 
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
Another gated cell proposed on 2014 is the Gated Recurrent Unit (GRU). It has similar advantages as the LSTM cell, but fewer parameters to train because the memory vector was removed and one of the gates. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
GRU has two gate functions &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{z},\mathbf{r}\in\mathcal{S}_{sig}
&lt;/script&gt;
&lt;/span&gt; named the &lt;i&gt;update&lt;/i&gt; and &lt;i&gt;reset&lt;/i&gt; gate respectively, and a state transition &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{s}\in\mathcal{S}_{tanh}.
&lt;/script&gt;
&lt;/span&gt; The input to GRU layer is only the tuple &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)
&lt;/script&gt;
&lt;/span&gt; and the output is &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt; which is being computed as follows: first the layer computes its gates for the current time step, denoted by &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{z}^{\left(t\right)}:=\mathbf{z}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)

&lt;/script&gt;
&lt;/span&gt;
 for the &lt;i&gt;update gate&lt;/i&gt; and by &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{r}^{\left(t\right)}:=\mathbf{r}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\right)

&lt;/script&gt;
&lt;/span&gt;
for the &lt;i&gt;reset gate&lt;/i&gt;. Then the output of the layer is computed by&lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{h}^{\left(t\right)}=\left(1-\mathbf{z}^{\left(t\right)}\right)\circ\mathbf{h}^{\left(t-1\right)}+\mathbf{z}^{\left(t\right)}\circ\mathbf{s}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\circ\mathbf{r}^{\left(t\right)}\right)

&lt;/script&gt;
&lt;/span&gt;
with all the arithmetic operations being done element wise. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
The term &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{s}\left(\mathbf{x}^{\left(t\right)},\mathbf{h}^{\left(t-1\right)}\circ\mathbf{r}^{\left(t\right)}\right)
&lt;/script&gt;
&lt;/span&gt; is a candidate for the next state. Note that the state-transition function accepts the previous state &lt;i&gt;moderated&lt;/i&gt; by the reset gate, allowing it to forget past states (therefore the name of the gate: reset). Then the output of the layer is a linear interpolation between the previous state and the candidate state, controlled by the update gate.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
As oppose to the LSTM cell, the GRU doesn’t have an output gate to control how much of its inner state to expose therefore the entire state is being exposed at each time step. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Both LSTM and GRU are very common in many recurrent network architectures and achieved great results in many tasks. LSTMs and GRUs can learn dependencies of various lengths which make the network very expressive. However, a too expressive network can leads sometimes to overfitting, to prevent that it is common to use some type of regularization, such as Dropout. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Next post I will discuss the dropout variations that are specific for RNNs.
&lt;/div&gt;</content><author><name>Eran Amar</name></author><category term="neural_networks" /><summary type="html">Warning: MathJax requires JavaScript to correctly process the mathematics on this page. Please enable JavaScript on your browser. &amp;lt;/hr&amp;gt;</summary></entry><entry><title type="html">Recurrent Neural Network - Introduction</title><link href="https://eranamar.github.io/site/2017/04/20/Recurrent-Neural-Network-Introduction.html" rel="alternate" type="text/html" title="Recurrent Neural Network - Introduction" /><published>2017-04-20T00:00:00+03:00</published><updated>2017-04-20T00:00:00+03:00</updated><id>https://eranamar.github.io/site/2017/04/20/Recurrent-Neural-Network---Introduction</id><content type="html" xml:base="https://eranamar.github.io/site/2017/04/20/Recurrent-Neural-Network-Introduction.html">&lt;script type=&quot;math/tex&quot;&gt;
\newcommand{\lyxlock}{}
&lt;/script&gt;

&lt;noscript&gt;
&lt;div class=&quot;warning&quot;&gt;
Warning: &lt;a href=&quot;http://www.mathjax.org/&quot;&gt;MathJax&lt;/a&gt; requires JavaScript to correctly process the mathematics on this page. Please enable JavaScript on your browser.
&lt;/div&gt;&lt;hr /&gt;
&amp;lt;/hr&amp;gt;&lt;/noscript&gt;

&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-1&quot;&gt;1&lt;/a&gt; The First Post in the Series
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
Recurrent Neural Network (RNN) is a neural network architecture that leads to state of the art results in many tasks, such as machine translation, speech recognition, image captioning and many more. Because it is a rapidly evolving field, it is difficult to find all the information (both basic and advanced material) in one place, so I decided to write a series of posts that will discuss Recurrent Neural Networks from their basic formulation to their most recent variations.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
This post, as the first in the series, will present the concept of a recurrent neuron and its limitations. Next posts will discuss gated neurons like LSTM and GRU, variants of dropout regularization for RNNs and some advanced stuff that I found interesting (bi-directional RNN, Multiplicative LSTM and etc). 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
The next section is dedicated to review the notations and basic terminology that will be used during the entire series. I will assume that the reader is familiar with the basics of neural network (fully connected architecture, back-propagation algorithm, etc). If you are not familiar with one of those concepts, I strongly recommend to review them online before going further. &lt;a class=&quot;URL&quot; href=&quot;https://www.academia.edu/25708860/A_Concise_Introduction_to_Machine_Learning_with_Artificial_Neural_Networks&quot;&gt;Those notes &lt;/a&gt; provides concise introduction to the field. There is also an excellent &lt;a class=&quot;URL&quot; href=&quot;https://www.youtube.com/watch?v=3BkFq55IEN8&amp;amp;feature=youtu.be&quot;&gt;video-lecture&lt;/a&gt; (in Hebrew) by Prof. Shai Shalev-Shwartz from 2016. If you really want to dive deeper, you can find broad introduction in Chapter 6 in &lt;i&gt;Deep Learning&lt;/i&gt; book (legally free electronic version &lt;a class=&quot;URL&quot; href=&quot;http://www.deeplearningbook.org/&quot;&gt;here&lt;/a&gt;).
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-2&quot;&gt;2&lt;/a&gt; Notations and Basic Terminology
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
Artificial Neural Network (ANN) is a mathematical hierarchal model inspired from neuroscience. The basic computation unit is a &lt;i&gt;neuron,&lt;/i&gt; which is a function from &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbb{R}^{d}
&lt;/script&gt;
&lt;/span&gt; to &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbb{R}
&lt;/script&gt;
&lt;/span&gt;, &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

g:\mathbf{x}\mapsto\sigma\left(\left\langle \mathbf{x},\mathbf{w}\right\rangle +b\right)

&lt;/script&gt;
&lt;/span&gt;
 where &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{x},\mathbf{w}\in\mathbb{R}^{d}
&lt;/script&gt;
&lt;/span&gt; are the &lt;i&gt;input&lt;/i&gt; and &lt;i&gt;weight&lt;/i&gt; vectors respectively, and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
b\in\mathbb{R}
&lt;/script&gt;
&lt;/span&gt; is the &lt;i&gt;bias&lt;/i&gt;. The function &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\sigma
&lt;/script&gt;
&lt;/span&gt; is a non-linear &lt;i&gt;activation&lt;/i&gt; function &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbb{R}\to\mathbb{R}
&lt;/script&gt;
&lt;/span&gt;, for example &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\sigma\left(z\right)=ReLU\left(z\right):=\max\left(z,0\right)
&lt;/script&gt;
&lt;/span&gt; or &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\sigma\left(z\right)=tanh\left(z\right):=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}
&lt;/script&gt;
&lt;/span&gt;. Note that a neuron can ignore some of its input coordinates by setting their corresponding entries in its weight vector to zero.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Neurons are organized within &lt;i&gt;layers&lt;/i&gt;. In the very basic ANN architecture, called a&lt;i&gt; fully connected (FC) neural network&lt;/i&gt;, a layer is just a collection of neurons that will be computed in parallel (meaning that they are not connected to each other). For any layer &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
i
&lt;/script&gt;
&lt;/span&gt;, each of its neurons accepts as inputs all neurons from layer &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
i-1
&lt;/script&gt;
&lt;/span&gt;. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
A layer in FC architecture can be formulate mathematically as a function from &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbb{R}^{d}
&lt;/script&gt;
&lt;/span&gt; to &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbb{R}^{n}
&lt;/script&gt;
&lt;/span&gt; such that &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{f}:\mathbf{x}\mapsto\left(\begin{array}{c}
g_{1}\left(\mathbf{x}\right)\\
\vdots\\
g_{n}\left(\mathbf{x}\right)
\end{array}\right)

&lt;/script&gt;
&lt;/span&gt;
 where each &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
g_{i}
&lt;/script&gt;
&lt;/span&gt; is a function of a neuron as defined above, with its own weight vector and a bias scalar. The formulation can be simplified using matrix notations: &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{f}:\mathbf{x}\mapsto\sigma\left(W\mathbf{x}+\mathbf{b}\right)

&lt;/script&gt;
&lt;/span&gt;
 where &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
W\in\mathbb{R}^{n\times d}
&lt;/script&gt;
&lt;/span&gt; contains in the &lt;i&gt;i&lt;/i&gt;-th row the weight vector of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
g_{i}
&lt;/script&gt;
&lt;/span&gt;, and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{b}_{i}
&lt;/script&gt;
&lt;/span&gt; is the bias of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
g_{i}
&lt;/script&gt;
&lt;/span&gt;. The function &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\sigma
&lt;/script&gt;
&lt;/span&gt; is the same as before but since its input is now a vector, it apply element-wise on each coordinate of the input, producing an output vector of the same length.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
We denote the family of all possible FC functions based on some fixed activation function as &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathcal{F}_{\sigma}=\left\{ \mathbf{x}\mapsto\sigma\left(W\mathbf{x}+\mathbf{b}\right)\mid\forall W,\mathbf{b}\right\} 

&lt;/script&gt;
&lt;/span&gt;
that family contains all possible layers for any input\output dimension. For abbreviation, I will not mention the dimensions of layers or the specific activation function &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\sigma
&lt;/script&gt;
&lt;/span&gt; when they are not important for the discussion. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Applying several layers sequentially yield a feedforward FC neural network. That is, given &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{x}\in\mathbb{R}^{d}
&lt;/script&gt;
&lt;/span&gt;, a FC network with &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt; layers is &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathcal{N}\left(\mathbf{x}\right)=\mathbf{f}_{t}\left(...\mathbf{f}_{2}\left(\mathbf{f}_{1}\left(\mathbf{x}\right)\right)\right)

&lt;/script&gt;
&lt;/span&gt;
 where &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\forall i:\:\mathbf{f}_{i}\in\mathcal{F}_{\sigma}
&lt;/script&gt;
&lt;/span&gt; for some fixed &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\sigma
&lt;/script&gt;
&lt;/span&gt;.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
The number of layers in the network is its &lt;i&gt;depth&lt;/i&gt;, and the size of the layers is its &lt;i&gt;width&lt;/i&gt; (assuming all the layers are of the same dimension). &lt;i&gt;Training&lt;/i&gt; the network is optimizing all the weight matrices and bias vectors such that the network will approximate some target (usually unknown) function. We can think of the optimization process as a process of iteratively updating the choice of layer functions &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{f}_{1},\mathbf{f}_{2},..
&lt;/script&gt;
&lt;/span&gt; from the family &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{F}_{\sigma}
&lt;/script&gt;
&lt;/span&gt;.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
The FC architecture is simple, yet yields a very powerful model. When wide enough, even a single hidden layer network can approximate to any arbitrary precision almost any continuous function (under some mild assumptions). However, for some tasks it might not be optimal, and different architectures are being used, instead of the FC one. 
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-3&quot;&gt;3&lt;/a&gt; The Need for Context
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
There are tasks that require some notion of &lt;i&gt;context&lt;/i&gt; in order to make a decent prediction. For example, tasks that involve time-steps can consider the history until the current time step as a context (a.k.a time-series tasks). For concrete example, consider predicting the next word in a given sequence. Each word has as a context all words preceding it. Note that there are more ways to define the notion of a context, and it mostly depends on the task in hand. For didactic purposes we will focus on the example of next-word prediction. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
How can we build a model for next-word prediction using FC neural network? 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
The obvious answer is to fix some window size &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
d
&lt;/script&gt;
&lt;/span&gt;, and build a network that gets the last &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
d
&lt;/script&gt;
&lt;/span&gt; words in the sentence as &lt;i&gt;features&lt;/i&gt;, in order to predict the next word. One immediate limitation of this approach is that the window size is the same for &lt;b&gt;all &lt;/b&gt;the examples in the training set, and it is not simple to find a suitable window size. On the one hand, choosing small window may cause the model to miss some important long term relations between words that are more than &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
d
&lt;/script&gt;
&lt;/span&gt; steps apart (for sentences longer than &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
d
&lt;/script&gt;
&lt;/span&gt;). On the other hand, choosing very large &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
d
&lt;/script&gt;
&lt;/span&gt; increases the number of parameters in the model, which make the optimization harder, and increase the computation time per example, which affect both training-time and prediction-time.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Another limitation is that FC layers regards their inputs as ordered information. That is, a FC layer assigns a specific weight for each coordinate in its input vector, causing the order of the features to be meaningful. But in some tasks the order of the features may vary, for example, the subject of a sentence doesn’t limit to specific location inside an English sentence. If we want FC layer to account for that, we have to get a lot of examples (and they need to cover all different cases). 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
What we would like to have here is called &lt;i&gt;parameter sharing&lt;/i&gt;. We will come back to that term later in this post.
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-4&quot;&gt;4&lt;/a&gt; Recurrent Cell - Learning to Remember
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
We want our network to be able to &lt;b&gt;remember&lt;/b&gt; information about previous invocations (i.e. the past inputs). In order to do that, our network have to maintain a state. So we need to extend our neuron formulation to include another variable, &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
h\in\mathbb{R}
&lt;/script&gt;
&lt;/span&gt;, that will be its state and be kept during consecutive invocations.That state variable is usually initialized to be zero so that it doesn’t affect the first computation.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
We will use an upper-script to denote the time step, so &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
h^{\left(t\right)}
&lt;/script&gt;
&lt;/span&gt; is the state of a neuron at time &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt;. Since the neurons need to pass information to themselves (i.e. their state from previous invocation) they become &lt;i&gt;recurrent neurons,&lt;/i&gt; a.k.a &lt;i&gt;recurrent cell&lt;/i&gt;s. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Recurrent neuron uses internally two functions. The first function updates the inner state based on &lt;b&gt;current input&lt;/b&gt; and &lt;b&gt;previous state&lt;/b&gt;, which is the transition from time &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t-1
&lt;/script&gt;
&lt;/span&gt; to time &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt;, &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

s:\mathbf{x},h^{\left(t-1\right)}\mapsto tanh\left(\left\langle \mathbf{x},\mathbf{w}\right\rangle +b+h^{\left(t-1\right)}\cdot u\right)

&lt;/script&gt;
&lt;/span&gt;
&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
u\in\mathbb{R}
&lt;/script&gt;
&lt;/span&gt; is a new parameter denoting the weight for the previous state.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
And the second function is for computing the output of the neuron at current time &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt;, based on its &lt;b&gt;new state&lt;/b&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

g^{\left(t\right)}:h^{\left(t\right)}\mapsto\sigma\left(h^{\left(t\right)}\cdot v+c\right)

&lt;/script&gt;
&lt;/span&gt;
with &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
v,c\in\mathbb{R}
&lt;/script&gt;
&lt;/span&gt; as the weight and bias of the computation respectively. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
We can move again to matrix notations to denote several parallel recurrent cells that forms a &lt;i&gt;recurrent layer&lt;/i&gt;. The first function updates the vector of states for the entire layer &lt;i&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{s}:\mathbf{x},\mathbf{h}^{\left(t-1\right)}\mapsto tanh\left(W\mathbf{x}+\mathbf{b}+U\mathbf{h}^{\left(t-1\right)}\right)

&lt;/script&gt;
&lt;/span&gt;
&lt;/i&gt; and the second function for computing the output of the layer based on the current state&lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{f}:\mathbf{h}^{\left(t\right)}\mapsto\sigma\left(V\mathbf{h}^{\left(t\right)}+\mathbf{c}\right)

&lt;/script&gt;
&lt;/span&gt;
When we come to compare that to the FC formulation, we see that actually the function &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{f}
&lt;/script&gt;
&lt;/span&gt; itself hasn’t changed, but now it accepts as input the state of the layer &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}
&lt;/script&gt;
&lt;/span&gt;, rather than the input to the layer &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{x}
&lt;/script&gt;
&lt;/span&gt;. Another change is that before the layer computes its output, it first update its hidden state.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Note that we can extend the definition of the family &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{F}_{tanh}
&lt;/script&gt;
&lt;/span&gt; to contains functions like the state-transforming function &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{s}
&lt;/script&gt;
&lt;/span&gt;, by making each &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{f}\in\mathcal{F}_{tanh}
&lt;/script&gt;
&lt;/span&gt; accepts two vectors as input instead of one (and we also need another weight matrix for the second input). So denote the family of all hidden-state transition functions as &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathcal{S}_{\sigma}=\left\{ \mathbf{x},\mathbf{h}^{\left(t-1\right)}\mapsto\sigma\left(W\mathbf{x}+\mathbf{b}+U\mathbf{h}^{\left(t-1\right)}\right)\mid\forall W,U,\mathbf{b}\right\} 

&lt;/script&gt;
&lt;/span&gt;
usually the activation function is &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\sigma=tanh
&lt;/script&gt;
&lt;/span&gt;. We will use that family later on, when discuss gated units as LSTM and GRU.
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-5&quot;&gt;5&lt;/a&gt; Comparing the Architectures - an Example
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
Consider again the next-word prediction problem. Suppose we have an ordered dictionary that contains the entire vocabulary we will ever need. We will represent any word by an integer indicating its index in the dictionary. In this section we will construct two single-neuron networks, once using the FC architecture and once using the Recurrent architecture.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
FC network for next-word prediction: Let the window size be &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
d=3
&lt;/script&gt;
&lt;/span&gt;. The input to the network can be represent as a vector &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{x}\in\mathbb{N}^{3}
&lt;/script&gt;
&lt;/span&gt; and the output as &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
y\in\mathbb{N}
&lt;/script&gt;
&lt;/span&gt;. The FC network amounts to &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathcal{N}_{FC}\left(\mathbf{x}\right)=f\left(x\right)

&lt;/script&gt;
&lt;/span&gt;
 for some &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
f\in\mathcal{F}_{\sigma}
&lt;/script&gt;
&lt;/span&gt; that maps from &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbb{R}^{3}
&lt;/script&gt;
&lt;/span&gt; to &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbb{R}
&lt;/script&gt;
&lt;/span&gt;. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Recurrent neural network (RNN), however, leads to different representation: the input and the output both integers &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
x,y\in\mathbb{N},
&lt;/script&gt;
&lt;/span&gt; so the state transition function &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
s\in\mathcal{S}
&lt;/script&gt;
&lt;/span&gt; and the output function &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
f\in\mathcal{F}_{\sigma}
&lt;/script&gt;
&lt;/span&gt; are both from &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbb{R}
&lt;/script&gt;
&lt;/span&gt; to &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbb{R}
&lt;/script&gt;
&lt;/span&gt;. The idea of the recurrent architecture is to apply the &lt;b&gt;same recurrent layer&lt;/b&gt; several times on &lt;b&gt;different inputs &lt;/b&gt;(each input from different time-step) in order to get a prediction. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Here is a detailed description of how to do that: we take a series of three consecutive words from the beginning of a sentence: &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
x^{\left(1\right)},x^{\left(2\right)},x^{\left(3\right)}
&lt;/script&gt;
&lt;/span&gt; and a target next word: &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
y
&lt;/script&gt;
&lt;/span&gt;. First, feed the network with &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
x^{\left(1\right)}
&lt;/script&gt;
&lt;/span&gt;, it updates the inner state from &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
h^{\left(0\right)}=0
&lt;/script&gt;
&lt;/span&gt; to some hidden state &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
h^{\left(1\right)}
&lt;/script&gt;
&lt;/span&gt; and outputs &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\hat{y}^{\left(1\right)}
&lt;/script&gt;
&lt;/span&gt;. We ignore that output and feed the next word &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
x^{\left(2\right)}
&lt;/script&gt;
&lt;/span&gt;, the network updates its state to &lt;b&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
h^{\left(2\right)}
&lt;/script&gt;
&lt;/span&gt; &lt;/b&gt;and produces &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\hat{y}^{\left(2\right)}
&lt;/script&gt;
&lt;/span&gt; that is also ignored. Lastly, we feed &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
x^{\left(3\right)}
&lt;/script&gt;
&lt;/span&gt; to the network, the inner state is being updated to &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
h^{\left(3\right)}
&lt;/script&gt;
&lt;/span&gt; and another output is produced &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\hat{y}^{\left(3\right)}
&lt;/script&gt;
&lt;/span&gt; which is now considered as the prediction of the network. Note that in each invocation of the network, the output depends on both the current input and the previous state, therefore &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\hat{y}^{\left(3\right)}
&lt;/script&gt;
&lt;/span&gt; is the result of the entire history &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
x^{\left(1\right)},x^{\left(2\right)},x^{\left(3\right)}
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
h^{\left(0\right)}
&lt;/script&gt;
&lt;/span&gt; (which was initialized to zero).
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
At first glance it looks like there is no meaningful differences between the architectures. We have an implicit sliding window over the words in the sentence for the recurrent neuron rather than explicit sliding window as with the FC neuron. Moreover, the output &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
y
&lt;/script&gt;
&lt;/span&gt; of FC neuron also depends on all previous time steps because the input &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{x}
&lt;/script&gt;
&lt;/span&gt; is just the values of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left(x^{\left(1\right)},x^{\left(2\right)},x^{\left(3\right)}\right)
&lt;/script&gt;
&lt;/span&gt;. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
However, note that in the FC architecture, each word in the input vector has its own weight and bias whereas in recurrent architecture we use the same weights over and over again (by applying the same function). In other words, in recurrent layers we &lt;i&gt;share the parameters across time&lt;/i&gt;. The formula below demonstrate how we applied the same function &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
s\in\mathcal{S}
&lt;/script&gt;
&lt;/span&gt; to updates the hidden-state across different time steps &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

h^{\left(t\right)}=s\left(x^{\left(t\right)},h^{\left(t-1\right)}\right)\Longrightarrow h^{\left(3\right)}=s\left(x^{\left(3\right)},s\left(x^{\left(2\right)},s\left(x^{\left(1\right)},0\right)\right)\right)

&lt;/script&gt;
&lt;/span&gt;
then making a prediction using some &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
f\in\mathcal{F}_{\sigma}
&lt;/script&gt;
&lt;/span&gt; on the current state&lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\hat{y}^{\left(3\right)}=f\left(h^{\left(3\right)}\right)

&lt;/script&gt;
&lt;/span&gt;
Parameter sharing has a very powerful effect on the learnability of the network both because it reduces the number of parameters that needed to be optimized and it make the network robust to small changes in the context (recall the example about the order of features in the input). In addition, the recurrent layer practically has a&lt;i&gt; dynamic&lt;/i&gt; window size. Going back to our next-word prediction network, we can continue to apply the network on consecutive inputs, teaching it dependencies between words that are further than 3 time steps. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Note that in our example, we ignored the “intermediate” predictions of the network: &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\hat{y}^{\left(1\right)}
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\hat{y}^{\left(2\right)}
&lt;/script&gt;
&lt;/span&gt;, but we could keep them and feed them as inputs to another recurrent layer making our RNN network deeper. We will discuss it in a later post.
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-6&quot;&gt;6&lt;/a&gt; Limitations of Recurrent Cells
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
Unfortunately, RNNs are not perfect and maintaining memory is not enough, sometimes the networks needs to be able to &lt;i&gt;forget&lt;/i&gt;, for instance, when predicting next-word in a sentence the network needs to reset its state once it reach the end of the sentence. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Furthermore, parameter sharing has a serious drawback. Recall that applying the network for multiple time steps leads to a composition of the same &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{s}\in\mathcal{S}
&lt;/script&gt;
&lt;/span&gt; upon itself &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt; times, something like this&lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{h}^{\left(t\right)}=\mathbf{s}\left(\mathbf{x}^{\left(t\right)},\mathbf{s}\left(\mathbf{x}^{\left(t-1\right)},\mathbf{s}\left(\mathbf{x}^{\left(t-2\right)},...\right)\right)\right)

&lt;/script&gt;
&lt;/span&gt;
 which may cause the gradient during back-propagation to either explode or vanish. This problem is known as &lt;i&gt;the&lt;/i&gt; &lt;i&gt;vanishing and exploding gradients&lt;/i&gt;. In order to get an intuition about that, we make the following simplifications: assume that the activation function inside &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{s}
&lt;/script&gt;
&lt;/span&gt; is the identity instead of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
tanh
&lt;/script&gt;
&lt;/span&gt;, and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{x}^{\left(t\right)}=\mathbf{0}
&lt;/script&gt;
&lt;/span&gt; for all &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt;. Suppose that &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}^{\left(0\right)}
&lt;/script&gt;
&lt;/span&gt; is given to us and it is not all zeros (that is, it encodes some information about the past). 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
We have that &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}^{\left(t\right)}=U\mathbf{h}^{\left(t-1\right)}
&lt;/script&gt;
&lt;/span&gt; so applying the recurrent neuron for &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt; time steps amounts to &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{h}^{\left(t\right)}=U^{t}\mathbf{h}^{\left(0\right)}
&lt;/script&gt;
&lt;/span&gt;. What happen to &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
U^{t}
&lt;/script&gt;
&lt;/span&gt; as &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt; increases? it easier to see the answer when considering the scalar case. If &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
U
&lt;/script&gt;
&lt;/span&gt; was a real positive number, then either &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
U^{t}\to0
&lt;/script&gt;
&lt;/span&gt; when &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
U&lt;1
&lt;/script&gt;
&lt;/span&gt;, or &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
U^{t}\to\infty
&lt;/script&gt;
&lt;/span&gt; when &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
U&gt;1
&lt;/script&gt;
&lt;/span&gt;. The same goes with matrices, suppose &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
U
&lt;/script&gt;
&lt;/span&gt; has an eigenvalue decomposition such that &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
U=V^{\top}\Sigma V
&lt;/script&gt;
&lt;/span&gt; for orthogonal &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
V
&lt;/script&gt;
&lt;/span&gt;, so &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
U^{t}=V^{\top}\Sigma^{t}V
&lt;/script&gt;
&lt;/span&gt; causing the small entries on the diagonal of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\Sigma
&lt;/script&gt;
&lt;/span&gt; to vanish, and the large entries to explode. Since the gradients in our network are being scaled according to &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\Sigma^{t}
&lt;/script&gt;
&lt;/span&gt;, they are either explode or vanished. The problem with very large gradients is that they cause the network to diverge (large updates to the parameters), on the other hand, very small gradients cause infinitesimally small updates to the parameters which prevent the network from learning. That is describe much more formally and rigorously &lt;a class=&quot;URL&quot; href=&quot;https://arxiv.org/abs/1211.5063&quot;&gt;here&lt;/a&gt;.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Next post I will discuss two popular improvements to the recurrent neurons that helps avoid the limitations above: the LSTM and GRU cells.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
&lt;b&gt;Acknowledgments:&lt;/b&gt; I want to thank Ron Shiff for his useful comments and corrections to this post.
&lt;/div&gt;</content><author><name>Eran Amar</name></author><category term="neural_networks" /><summary type="html">Warning: MathJax requires JavaScript to correctly process the mathematics on this page. Please enable JavaScript on your browser. &amp;lt;/hr&amp;gt;</summary></entry><entry><title type="html">Memory Efficient Algorithm for Finding Cycles in Graphs</title><link href="https://eranamar.github.io/site/2017/04/06/Memory-Efficient-Algorithm-for-Finding-Cycles-in-Graphs.html" rel="alternate" type="text/html" title="Memory Efficient Algorithm for Finding Cycles in Graphs" /><published>2017-04-06T00:00:00+03:00</published><updated>2017-04-06T00:00:00+03:00</updated><id>https://eranamar.github.io/site/2017/04/06/Memory-Efficient-Algorithm-for-Finding-Cycles-in-Graphs</id><content type="html" xml:base="https://eranamar.github.io/site/2017/04/06/Memory-Efficient-Algorithm-for-Finding-Cycles-in-Graphs.html">&lt;script type=&quot;math/tex&quot;&gt;
\newcommand{\lyxlock}{}
&lt;/script&gt;

&lt;noscript&gt;
&lt;div class=&quot;warning&quot;&gt;
Warning: &lt;a href=&quot;http://www.mathjax.org/&quot;&gt;MathJax&lt;/a&gt; requires JavaScript to correctly process the mathematics on this page. Please enable JavaScript on your browser.
&lt;/div&gt;&lt;hr /&gt;
&amp;lt;/hr&amp;gt;&lt;/noscript&gt;

&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-1&quot;&gt;1&lt;/a&gt; Cycle Detection in Graphs 
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
Somewhere in 2015, I encountered a nice algorithmic question during an interview for some algotreading company. The question was &lt;i&gt;how one can decide if there is a cycle in an undirected connected graph&lt;/i&gt;? There are two immediate answers here, the first is running &lt;i&gt;Breadth First Search (BFS)&lt;/i&gt; on arbitrary node and memorizing the nodes that were already visited. The search ends either when the algorithm complete scanning the whole graph or when it reach some node twice, outputting “No” or “Yes” respectively. Another possible solution is by running &lt;i&gt;Depth First Search (DFS)&lt;/i&gt; and keeping a variable to count the length of the path explored. The algorithm declare a cycle if the length of the path reach the number of vertices in the graph. For the rest of this post, denote that number by &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
n
&lt;/script&gt;
&lt;/span&gt;.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Those two solutions are simple, and more importantly, have the same &lt;i&gt;space complexity&lt;/i&gt;. That is, both consumes around the same amount of working memory (in bits) ignoring the memory needed for the input and output of the algorithm. Even though DFS have implementation that requires less memory than BFS, ultimately both requires &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\Omega\left(n\log n\right)
&lt;/script&gt;
&lt;/span&gt; bits. One can justify the space complexity as follow: both algorithms has to keep track of their search (a queue in BFS and a stack is DFS) which potentially can be as large as the entire graph thus &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\Omega\left(n\right)
&lt;/script&gt;
&lt;/span&gt;, and each node need &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\Omega\left(\log n\right)
&lt;/script&gt;
&lt;/span&gt; bits for encoding so the total space complexity is &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\Omega\left(n\log n\right)
&lt;/script&gt;
&lt;/span&gt; bits.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
During the rest of this post we will focus on &lt;i&gt;space complexity&lt;/i&gt; rather than &lt;i&gt;time complexity&lt;/i&gt;. We will present an algorithm that can determine if there is a cycle in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{O}\left(\log^{2}n\right)
&lt;/script&gt;
&lt;/span&gt; bits [base on &lt;a class=&quot;URL&quot; href=&quot;http://www.sciencedirect.com/science/article/pii/S002200007080006X&quot;&gt;this paper&lt;/a&gt;] and then discuss the importance of related problem: the &lt;i&gt;s-t connectivity&lt;/i&gt;.
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-2&quot;&gt;2&lt;/a&gt; The &lt;i&gt;s-t Connectivity&lt;/i&gt; Problem
&lt;/h1&gt;
&lt;h2 class=&quot;Subsection&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Subsection-2.1&quot;&gt;2.1&lt;/a&gt; Solving &lt;i&gt;s-t connectivity&lt;/i&gt; as a Sub-step
&lt;/h2&gt;
&lt;div class=&quot;Unindented&quot;&gt;
We will start from solving a slightly different problem then uses its solution as a procedure in our cycle-detection algorithm. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Introducing the &lt;i&gt;undirected s-t connectivity&lt;/i&gt; problem: The input is an undirected graph &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
G=\left(V,E\right)
&lt;/script&gt;
&lt;/span&gt; of size &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
n
&lt;/script&gt;
&lt;/span&gt; and two vertices &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
s,t\in V
&lt;/script&gt;
&lt;/span&gt;. The output should be &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
1
&lt;/script&gt;
&lt;/span&gt; if there is path in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
G
&lt;/script&gt;
&lt;/span&gt; that connects &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
s
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt;, and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
0
&lt;/script&gt;
&lt;/span&gt; otherwise. The &lt;i&gt;directed s-t connectivity&lt;/i&gt; is the about the same but the input graph is directed (i.e its edges has directions) and the output should be &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
1
&lt;/script&gt;
&lt;/span&gt; whether there is a &lt;i&gt;directed&lt;/i&gt; path from &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
s
&lt;/script&gt;
&lt;/span&gt; to &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt;.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Before we present the algorithm, we first define the following predicate: for any &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
u,v\in V
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
k\in\mathbb{N}
&lt;/script&gt;
&lt;/span&gt; define &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
conn\left(u,v;k\right)=1
&lt;/script&gt;
&lt;/span&gt; if exists a path from &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
u
&lt;/script&gt;
&lt;/span&gt; to &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
v
&lt;/script&gt;
&lt;/span&gt; of at most &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
k
&lt;/script&gt;
&lt;/span&gt; steps (i.e. edges), otherwise &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
conn\left(u,v;k\right)=0
&lt;/script&gt;
&lt;/span&gt;. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
The next observations are key points needed to solve the &lt;i&gt;s-t connectivity &lt;/i&gt;efficiently (both for directed and undirected case).
&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;
For any &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
u,v\in V
&lt;/script&gt;
&lt;/span&gt; and any &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
k\in\mathbb{N}
&lt;/script&gt;
&lt;/span&gt;, we have that &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
conn\left(u,v;2^{k}\right)=1
&lt;/script&gt;
&lt;/span&gt; if and only if &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\exists w\in V
&lt;/script&gt;
&lt;/span&gt; such that &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
conn\left(u,w;2^{k-1}\right)=1
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
conn\left(w,v;2^{k-1}\right)=1
&lt;/script&gt;
&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
Let &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
u,v\in V
&lt;/script&gt;
&lt;/span&gt;, they are connected if and only if &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
conn\left(u,v;2^{\lceil\log n\rceil}\right)=1
&lt;/script&gt;
&lt;/span&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;Unindented&quot;&gt;
From here the algorithm is straight forward: denote the algorithm by &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
Alg\left(u,v;2^{k}\right)
&lt;/script&gt;
&lt;/span&gt; that upon receiving &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
u,v
&lt;/script&gt;
&lt;/span&gt; from the graph &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
G=\left(V,E\right)
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
k\in\mathbb{N}
&lt;/script&gt;
&lt;/span&gt;:
&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;
If &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
k=0
&lt;/script&gt;
&lt;/span&gt; output &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{1}_{\left[u=v\right]}
&lt;/script&gt;
&lt;/span&gt;, and if &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
k=1
&lt;/script&gt;
&lt;/span&gt; output &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{1}_{\left[u=v\vee\left(u,v\right)\in E\right]}
&lt;/script&gt;
&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
For each &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
w\in V
&lt;/script&gt;
&lt;/span&gt;: if &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
Alg\left(u,w;2^{k-1}\right)=1
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
Alg\left(w,v;2^{k-1}\right)=1
&lt;/script&gt;
&lt;/span&gt; then output &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
1
&lt;/script&gt;
&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
Output &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
0
&lt;/script&gt;
&lt;/span&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;Unindented&quot;&gt;
The solution for &lt;i&gt;s-t connectivity&lt;/i&gt; is achieved by outputting “Yes” if and only if &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
Alg\left(s,t,2^{\lceil\log n\rceil}\right)=1
&lt;/script&gt;
&lt;/span&gt;. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Obviously, the depth of the recursion is &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{O}\left(\log n\right)
&lt;/script&gt;
&lt;/span&gt; and in each level of the recursion the algorithm only needs to keep track of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
w
&lt;/script&gt;
&lt;/span&gt;, which is &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{O}\left(\log n\right)
&lt;/script&gt;
&lt;/span&gt; bits, therefore the total memory complexity of the algorithm is &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{O}\left(\log^{2}n\right)
&lt;/script&gt;
&lt;/span&gt; bits.
&lt;/div&gt;
&lt;h2 class=&quot;Subsection&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Subsection-2.2&quot;&gt;2.2&lt;/a&gt; Extending the algorithm to detect cycles
&lt;/h2&gt;
&lt;div class=&quot;Unindented&quot;&gt;
How solving the &lt;i&gt;s-t connectivity&lt;/i&gt; problem helps us with cycle detection? The next observation answer that: let &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
G=\left(V,E\right)
&lt;/script&gt;
&lt;/span&gt; be the graph in question and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
u,v\in V
&lt;/script&gt;
&lt;/span&gt; two endpoints of some edge, they must stay connected after removing that edge from &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
G
&lt;/script&gt;
&lt;/span&gt; if and only if both participating in a cycle contained in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
G
&lt;/script&gt;
&lt;/span&gt;.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
For the directed case the algorithm is as follow: iterate over any edge &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left(u,v\right)\in E
&lt;/script&gt;
&lt;/span&gt;, delete it temporarily from &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
G
&lt;/script&gt;
&lt;/span&gt; and check if &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
Alg\left(v,u;2^{\lceil\log n\rceil}\right)=1
&lt;/script&gt;
&lt;/span&gt; on the resulting graph, that is, if &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
v
&lt;/script&gt;
&lt;/span&gt; is still connected to &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
u
&lt;/script&gt;
&lt;/span&gt; even after you delete the edge from &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
u
&lt;/script&gt;
&lt;/span&gt; to &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
v
&lt;/script&gt;
&lt;/span&gt;. If &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
Alg\left(v,u;2^{\lceil\log n\rceil}\right)=1
&lt;/script&gt;
&lt;/span&gt; then declare a cycle, else, restore back the edge and continue to the next one. Note that this solution also works for undirected graphs. The memory complexity is &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{O}\left(\log^{2}n\right)+\log n=\mathcal{O}\left(\log^{2}n\right)
&lt;/script&gt;
&lt;/span&gt; and we done.
&lt;/div&gt;
&lt;h2 class=&quot;Subsection&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Subsection-2.3&quot;&gt;2.3&lt;/a&gt; Is it the best we can do?
&lt;/h2&gt;
&lt;div class=&quot;Unindented&quot;&gt;
Actually, for undirected graphs there is an algorithm with &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{O}\left(\log n\right)
&lt;/script&gt;
&lt;/span&gt; bits, improving the square on the log from the construction from previous section. The interested reader can read more about it &lt;a class=&quot;URL&quot; href=&quot;http://www.cs.cornell.edu/courses/cs682/2008sp/Handouts/Reingold05.pdf&quot;&gt;here&lt;/a&gt;. For the directed case it is an open question whether we can improve the &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\log^{2}n
&lt;/script&gt;
&lt;/span&gt; or not.
&lt;/div&gt;
&lt;h2 class=&quot;Subsection&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Subsection-2.4&quot;&gt;2.4&lt;/a&gt; Beyond Cycles in Graphs
&lt;/h2&gt;
&lt;div class=&quot;Unindented&quot;&gt;
The &lt;i&gt;directed s-t connectivity &lt;/i&gt;problem is important because it captures the “hardness” of any problem that requires logarithmic amount of memory space. The reduction to show that is actually very easy, suppose we have an algorithm that solves some problem in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{O}\left(s\right)
&lt;/script&gt;
&lt;/span&gt; space where &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
s
&lt;/script&gt;
&lt;/span&gt; is logarithmic in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
n
&lt;/script&gt;
&lt;/span&gt; (the size of the input), generate the following directed graph: create a vertex &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
v
&lt;/script&gt;
&lt;/span&gt; for any state (memory configuration) of the algorithm. Since its memory is &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{O}\left(s\right)
&lt;/script&gt;
&lt;/span&gt;, there are at most &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
2^{\mathcal{O}\left(s\right)}=\mathcal{O}\left(poly\left(n\right)\right)
&lt;/script&gt;
&lt;/span&gt; states (thus vertices). Create an edge from &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
u
&lt;/script&gt;
&lt;/span&gt; to &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
v
&lt;/script&gt;
&lt;/span&gt; iff &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
v
&lt;/script&gt;
&lt;/span&gt; is a valid possible successor state of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
u
&lt;/script&gt;
&lt;/span&gt;. Add a vertex &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt; and connects to it all the states that leads to outputting “Yes”. Denote by &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
s
&lt;/script&gt;
&lt;/span&gt; the starting-state of the algorithm and now solving the original problem is equivalent to finding a directed path from &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
s
&lt;/script&gt;
&lt;/span&gt; to &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt; in the states-graph.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
This reduction was presented by Walter Savitch in 1970, and it also works for Nondeterministic algorithms. The conclusion is that any nondeterministic problem that can be solved in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{O}\left(s\right)
&lt;/script&gt;
&lt;/span&gt; space (where &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
s
&lt;/script&gt;
&lt;/span&gt; is logarithmic in the size of the input) can be solved deterministically in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{O}\left(s^{2}\right)
&lt;/script&gt;
&lt;/span&gt;. Applying that conclusion with the algorithm for &lt;i&gt;directed s-t connectivity&lt;/i&gt; yields that any problem with nondeterministic &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{O}\left(\log n\right)
&lt;/script&gt;
&lt;/span&gt; memory can be solved deterministically (via reduction to &lt;i&gt;directed s-t connectivity&lt;/i&gt;) with &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{O}\left(\log^{2}n\right)
&lt;/script&gt;
&lt;/span&gt; memory.
&lt;/div&gt;</content><author><name>Eran Amar</name></author><category term="complexity" /><category term="algorithms" /><summary type="html">Warning: MathJax requires JavaScript to correctly process the mathematics on this page. Please enable JavaScript on your browser. &amp;lt;/hr&amp;gt;</summary></entry><entry><title type="html">Derandomization for Pairwise Independent Seed</title><link href="https://eranamar.github.io/site/2017/03/30/Derandomization-for-Pairwise-Independent-Seed.html" rel="alternate" type="text/html" title="Derandomization for Pairwise Independent Seed" /><published>2017-03-30T00:00:00+03:00</published><updated>2017-03-30T00:00:00+03:00</updated><id>https://eranamar.github.io/site/2017/03/30/Derandomization-for-Pairwise-Independent-Seed</id><content type="html" xml:base="https://eranamar.github.io/site/2017/03/30/Derandomization-for-Pairwise-Independent-Seed.html">&lt;script type=&quot;math/tex&quot;&gt;
\newcommand{\lyxlock}{}
&lt;/script&gt;

&lt;noscript&gt;
&lt;div class=&quot;warning&quot;&gt;
Warning: &lt;a href=&quot;http://www.mathjax.org/&quot;&gt;MathJax&lt;/a&gt; requires JavaScript to correctly process the mathematics on this page. Please enable JavaScript on your browser.
&lt;/div&gt;&lt;hr /&gt;
&amp;lt;/hr&amp;gt;&lt;/noscript&gt;

&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-1&quot;&gt;1&lt;/a&gt; Randomized Algorithm Settings
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
Randomized algorithm is an algorithm that in addition to its input also uses internally &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt; coin-tosses (i.e. random bits) in order to decide on its output, so different invocations of the algorithm with the same input may yield different outputs that depends on the results of the coins. One can think about the internal coin-tosses as a binary string &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
r
&lt;/script&gt;
&lt;/span&gt; of length &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt; that is being fixed before the algorithm starts, a.k.a &lt;i&gt;seed&lt;/i&gt;, and the randomness now comes from choosing different string for each invocation. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Note that tossing &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt; fair coins, is exactly like drawing a random string &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
r
&lt;/script&gt;
&lt;/span&gt; from the set &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left\{ 0,1\right\} ^{t}
&lt;/script&gt;
&lt;/span&gt; where all candidates have equal probability (i.e. each has a probability of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
2^{-t}
&lt;/script&gt;
&lt;/span&gt;).
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Say we have a randomize algorithm &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
A
&lt;/script&gt;
&lt;/span&gt; for some problem that we would like to solve. We have a guarantee that for any input &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
x
&lt;/script&gt;
&lt;/span&gt; of length &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
n
&lt;/script&gt;
&lt;/span&gt;, algorithm &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
A
&lt;/script&gt;
&lt;/span&gt; runs in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
poly\left(n\right)
&lt;/script&gt;
&lt;/span&gt;, and outputs either a valid solution &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
S
&lt;/script&gt;
&lt;/span&gt; if it succeeds or output &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
failed
&lt;/script&gt;
&lt;/span&gt;. The success probability of the algorithm is &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
p
&lt;/script&gt;
&lt;/span&gt; (which may be even infinitesimally small). 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
The first question to ask is, &lt;i&gt;can we convert the randomized algorithm into a deterministic one?&lt;/i&gt;
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-2&quot;&gt;2&lt;/a&gt; Brute Force Derandomization
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
The answer is&lt;i&gt; yes&lt;/i&gt;, and there is a simple way to do that: Upon receiving an input &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
x
&lt;/script&gt;
&lt;/span&gt;, iterate over all possible strings &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
r\in\left\{ 0,1\right\} ^{t}
&lt;/script&gt;
&lt;/span&gt; and run &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
A_{r}\left(x\right)
&lt;/script&gt;
&lt;/span&gt;. That is, simulate &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
A
&lt;/script&gt;
&lt;/span&gt; on &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
x
&lt;/script&gt;
&lt;/span&gt; with &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
r
&lt;/script&gt;
&lt;/span&gt; as the random seed. Return the first solution that &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
A
&lt;/script&gt;
&lt;/span&gt; outputs (i.e. anything that is not &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
failed
&lt;/script&gt;
&lt;/span&gt;).
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
The idea behind this method is that if there is a non-zero probability for &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
A
&lt;/script&gt;
&lt;/span&gt; to output a valid solution for the input &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
x
&lt;/script&gt;
&lt;/span&gt;, then there must be a random seed that leads to that output when executing algorithm &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
A
&lt;/script&gt;
&lt;/span&gt; on &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
x
&lt;/script&gt;
&lt;/span&gt;. So all we need to do is iterating over all possible seeds of length &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt;, and that leads to a deterministic algorithm. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
You probably already see the problem here. If &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t=\mathcal{O}\left(n\right)
&lt;/script&gt;
&lt;/span&gt; then the running time of the deterministic procedure is &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
2^{\mathcal{O}\left(n\right)}\cdot poly\left(n\right)
&lt;/script&gt;
&lt;/span&gt;.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Terrible, terrible running time.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
On the other hand, if &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt; is a constant or even up to &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{O}\left(\log n\right)
&lt;/script&gt;
&lt;/span&gt;, then the resulting running time will stay polynomial, and everybody is happy. In the next section we will see that with an additional assumption, we can “decrease” the seed length from &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
n
&lt;/script&gt;
&lt;/span&gt; to &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\log n
&lt;/script&gt;
&lt;/span&gt; bits.
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-3&quot;&gt;3&lt;/a&gt; Pairwise Independent Seed
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
We start off with a definition.
&lt;/div&gt;
&lt;div class=&quot;Definition&quot;&gt;
&lt;i&gt;k&lt;/i&gt;-wise Independent Family. Let &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{X}
&lt;/script&gt;
&lt;/span&gt; be a set of random variables. We say that &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{X}
&lt;/script&gt;
&lt;/span&gt; is a &lt;i&gt;k&lt;/i&gt;-wise independent family if for any subset of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
k
&lt;/script&gt;
&lt;/span&gt; variables &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
X_{1},..,X_{k}\in\mathcal{X}
&lt;/script&gt;
&lt;/span&gt;, and values &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
y_{1},...,y_{k}
&lt;/script&gt;
&lt;/span&gt; &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{P}\left[\bigwedge_{i\in I}X_{i}=y_{i}\right]=\prod_{i\in I}\mathbf{P}\left[X_{i}=y_{i}\right]\qquad\forall I\subseteq\left[k\right]

&lt;/script&gt;
&lt;/span&gt;
that is, any subset of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
k
&lt;/script&gt;
&lt;/span&gt; random variables from &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{X}
&lt;/script&gt;
&lt;/span&gt; is fully independent. The family &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{X}
&lt;/script&gt;
&lt;/span&gt; is called pairwise independent family when &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
k=2
&lt;/script&gt;
&lt;/span&gt;.
&lt;/div&gt;
&lt;div class=&quot;Unindented&quot;&gt;

&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
What this definition have with our settings? observe that up until now we implicitly assumed that the algorithm needs its coins to be fully independent. That is, if we think of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
s
&lt;/script&gt;
&lt;/span&gt; as a series of Bernoulli r.vs then we actually required &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
s
&lt;/script&gt;
&lt;/span&gt; to be &lt;i&gt;t&lt;/i&gt;-wise independent family, which is much stronger property than pairwise independence. If we relax that requirement and assume that the algorithm only needs its coins to be pairwise independent, then there is a way (described in next section) to construct &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
n
&lt;/script&gt;
&lt;/span&gt; &lt;i&gt;pairwise independent&lt;/i&gt; bits using only &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{O}\left(\log n\right)
&lt;/script&gt;
&lt;/span&gt; &lt;i&gt;fully independent&lt;/i&gt; bits. That construction can be “embedded” into our algorithm such that we ultimately ends up with a procedure that only requires &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{O}\left(\log n\right)
&lt;/script&gt;
&lt;/span&gt; fully independent seed. Applying the derandomization method from previous section will result with a deterministic algorithm that will still run in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
poly\left(n\right)
&lt;/script&gt;
&lt;/span&gt; time.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Of course, we may assume pairwise independence only if it won’t break the correctness analysis of the randomized algorithm that we want to derandomize.
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-4&quot;&gt;4&lt;/a&gt; Stretching the Seed
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
Suppose we have &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\approx\log n
&lt;/script&gt;
&lt;/span&gt; fair fully independent coins, we want to construct a method to generate from them &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\approx n
&lt;/script&gt;
&lt;/span&gt; fair pairwise independent bits. The following claim shows such construction.
&lt;/div&gt;
&lt;div class=&quot;Claim&quot;&gt;
Suppose we have &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t:=\log\left(n\right)+1
&lt;/script&gt;
&lt;/span&gt; fully independent Bernoulli random variables, each with probability &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\frac{1}{2}
&lt;/script&gt;
&lt;/span&gt;. Fix them in some arbitrary order to get a (random) vector &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{s}
&lt;/script&gt;
&lt;/span&gt; of length &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt;. Let &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{S}=\left\{ \left\langle \mathbf{s},\mathbf{x}\right\rangle _{mod2}\mid\mathbf{x}\in\left\{ 0,1\right\} ^{t},\:\:x\ne\mathbf{0}\right\} 
&lt;/script&gt;
&lt;/span&gt; be the set of all possible sums modulo 2 of those r.vs and note that &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left|\mathcal{S}\right|=2^{t}-1=\Theta\left(n\right)
&lt;/script&gt;
&lt;/span&gt;, then &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{S}
&lt;/script&gt;
&lt;/span&gt; is a family of pairwise independent bits.
&lt;/div&gt;
&lt;div class=&quot;Unindented&quot;&gt;
To prove that we start with a very basic lemma.
&lt;/div&gt;
&lt;div class=&quot;Lemma&quot;&gt;
Let &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
X,Y\sim Bernoulli\left(0.5\right)
&lt;/script&gt;
&lt;/span&gt; be two independent random variables, let &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
Z=X+Y
&lt;/script&gt;
&lt;/span&gt; then &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
Z\sim Bernoulli\left(0.5\right)
&lt;/script&gt;
&lt;/span&gt;. Moreover, &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
Z
&lt;/script&gt;
&lt;/span&gt; is independent from &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
X
&lt;/script&gt;
&lt;/span&gt; and from &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
Y
&lt;/script&gt;
&lt;/span&gt;.
&lt;/div&gt;
&lt;div class=&quot;Proof&quot;&gt;
For any &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
b\in\left\{ 0,1\right\} 
&lt;/script&gt;
&lt;/span&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;
\begin{aligned}
\mathbf{P}\left[X+Y=b\right] &amp; =\mathbf{P}\left[X=1,Y=1-b\right]+\mathbf{P}\left[X=0,Y=b\right]\\
 &amp; =\mathbf{P}\left[X=1\right]\mathbf{P}\left[Y=1-b\right]+\mathbf{P}\left[X=0\right]\mathbf{P}\left[Y=b\right]\\
 &amp; =\frac{1}{2}\left(\mathbf{P}\left[Y=1-b\right]+\mathbf{P}\left[Y=1-b\right]\right)\\
 &amp; =\frac{1}{2}
\end{aligned}
&lt;/script&gt;
&lt;/span&gt;
thus &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
Z\sim Bernoulli\left(0.5\right)
&lt;/script&gt;
&lt;/span&gt;. Next w.l.o.g we prove that &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
Z
&lt;/script&gt;
&lt;/span&gt; independent from &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
X
&lt;/script&gt;
&lt;/span&gt;. Take any &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
b_{1},b_{2}\in\left\{ 0,1\right\} 
&lt;/script&gt;
&lt;/span&gt;. If &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
b_{1}=b_{2}
&lt;/script&gt;
&lt;/span&gt; then &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;
\begin{aligned}
\mathbf{P}\left[X=b_{1},Z=b_{2}\right] &amp; =\mathbf{P}\left[X=b_{1},X+Y=b_{1}\right]\\
 &amp; =\mathbf{P}\left[X=b_{1},Y=0\right]\\
 &amp; =\mathbf{P}\left[X=b_{1}\right]\mathbf{P}\left[Y=0\right]\\
 &amp; =\mathbf{P}\left[X=b_{1}\right]\cdot\frac{1}{2}\\
 &amp; =\mathbf{P}\left[X=b_{1}\right]\mathbf{P}\left[Z=b_{1}\right]
\end{aligned}
&lt;/script&gt;
&lt;/span&gt;
and we done. And if &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
b_{2}=1-b_{1}
&lt;/script&gt;
&lt;/span&gt; then very similarly we get&lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;
\begin{aligned}
\mathbf{P}\left[X=b_{1},Z=b_{2}\right] &amp; =\mathbf{P}\left[X=b_{1},X+Y=1-b_{1}\right]\\
 &amp; =\mathbf{P}\left[X=b_{1},Y=1\right]\\
 &amp; =\mathbf{P}\left[X=b_{1}\right]\mathbf{P}\left[Z=b_{1}\right]
\end{aligned}
&lt;/script&gt;
&lt;/span&gt;
which conclude the proof.
&lt;/div&gt;
&lt;div class=&quot;Unindented&quot;&gt;
Now we prove the claim from above.
&lt;/div&gt;
&lt;div class=&quot;Proof&quot;&gt;
Take two different variables &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
X_{1}=\left\langle \mathbf{s},\mathbf{x}_{1}\right\rangle \in\mathcal{S}
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
X_{2}=\left\langle \mathbf{s},\mathbf{x}_{2}\right\rangle \in\mathcal{S}
&lt;/script&gt;
&lt;/span&gt;, and let &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
b_{1},b_{2}\in\left\{ 0,1\right\} 
&lt;/script&gt;
&lt;/span&gt;, we want to show that &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
X_{1}
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
X_{2}
&lt;/script&gt;
&lt;/span&gt; are independent. Obviously, we only care about the set of indices which are non-zero in the vectors &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{x}_{1}
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{x}_{2}
&lt;/script&gt;
&lt;/span&gt;. Let these sets be &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
I_{1},I_{2}
&lt;/script&gt;
&lt;/span&gt; respectively. Denote &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
M_{0}=\sum_{i\in I_{1}\cap I_{2}}\mathbf{s}_{i}
&lt;/script&gt;
&lt;/span&gt;, &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
M_{1}=\sum_{i\in I_{1}\backslash I_{2}}\mathbf{s}_{i}
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
M_{2}=\sum_{i\in I_{2}\backslash I_{1}}\mathbf{s}_{i}
&lt;/script&gt;
&lt;/span&gt; where summation over empty set is defined as the constant &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
0
&lt;/script&gt;
&lt;/span&gt; (all the operations in the proof are modulo 2). Note that any pair of variables from &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left\{ M_{0},M_{1},M_{2}\right\} 
&lt;/script&gt;
&lt;/span&gt; are independent because they defined on mutually disjoint set of indices. Moreover, by the lemma from above, each of them if it is not a constant zero then it is distributed as &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
Bernoulli\left(0.5\right)
&lt;/script&gt;
&lt;/span&gt;. When plugging in the new notations we get&lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;
\begin{aligned}
\mathbf{P}\left[X_{1}=b_{1},\:X_{2}=b_{2}\right] &amp; =\mathbf{P}\left[M_{0}+M_{1}=b_{1},\:M_{0}+M_{2}=b_{2}\right]
\end{aligned}
&lt;/script&gt;
&lt;/span&gt;
Because &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
I_{1}\ne I_{2}
&lt;/script&gt;
&lt;/span&gt; there must be at most one variable from &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left\{ M_{0},M_{1},M_{2}\right\} 
&lt;/script&gt;
&lt;/span&gt; which may be constant. Therefore, no matter which &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
M_{i}
&lt;/script&gt;
&lt;/span&gt; it is (if any), either &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
X_{1}
&lt;/script&gt;
&lt;/span&gt; or &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
X_{2}
&lt;/script&gt;
&lt;/span&gt; is a sum of two non-constant variables. W.l.o.g let it be &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
X_{1}
&lt;/script&gt;
&lt;/span&gt;, then it has the &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
M_{1}
&lt;/script&gt;
&lt;/span&gt; component which does not appear in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
X_{2}
&lt;/script&gt;
&lt;/span&gt; and can flip the result of the sum with probability &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
0.5
&lt;/script&gt;
&lt;/span&gt; &lt;b&gt;regardless&lt;/b&gt; of what happen anywhere else, therefore &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
X_{1}
&lt;/script&gt;
&lt;/span&gt; is independent of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
X_{2}
&lt;/script&gt;
&lt;/span&gt;, and that complete the proof.
&lt;/div&gt;
&lt;div class=&quot;Unindented&quot;&gt;

&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Last one remark. In our settings the randomized algorithm either return a solution or says “failed”, if we have a polynomial procedure to validate a solution we can generalize the settings to the case when the algorithm always output a solution, and then we verify it to determine if it is “succeeds” case of “failed” case.
&lt;/div&gt;</content><author><name>Eran Amar</name></author><category term="probability_theory" /><category term="algorithms" /><summary type="html">Warning: MathJax requires JavaScript to correctly process the mathematics on this page. Please enable JavaScript on your browser. &amp;lt;/hr&amp;gt;</summary></entry><entry><title type="html">Comparing Chernoff - Hoeffding bounds</title><link href="https://eranamar.github.io/site/2017/03/24/Comparing-Chernoff-Hoeffding-bounds.html" rel="alternate" type="text/html" title="Comparing Chernoff - Hoeffding bounds" /><published>2017-03-24T00:00:00+02:00</published><updated>2017-03-24T00:00:00+02:00</updated><id>https://eranamar.github.io/site/2017/03/24/Comparing-Chernoff---Hoeffding-bounds</id><content type="html" xml:base="https://eranamar.github.io/site/2017/03/24/Comparing-Chernoff-Hoeffding-bounds.html">&lt;script type=&quot;math/tex&quot;&gt;
\newcommand{\lyxlock}{}
&lt;/script&gt;

&lt;noscript&gt;
&lt;div class=&quot;warning&quot;&gt;
Warning: &lt;a href=&quot;http://www.mathjax.org/&quot;&gt;MathJax&lt;/a&gt; requires JavaScript to correctly process the mathematics on this page. Please enable JavaScript on your browser.
&lt;/div&gt;&lt;hr /&gt;
&amp;lt;/hr&amp;gt;&lt;/noscript&gt;

&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-1&quot;&gt;1&lt;/a&gt; The Definitions and Motivation
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
Hoeffding and Chernoff bounds (a.k.a “inequalities”) are very common &lt;i&gt;concentration measures&lt;/i&gt; that are being used in many fields in computer science. A concentration measure is a way to bound the probability of the sum of random variables to get values outside a neighborhood of the sum of their means. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
For example: suppose we have &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
n
&lt;/script&gt;
&lt;/span&gt; (fair) coins to toss and we want to know what is the probability that the total number of heads (i.e sum of indicators) will be “far” from half of the tosses (which is the mean of that sum), say that either &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\#heads&gt;\frac{3}{4}n
&lt;/script&gt;
&lt;/span&gt; or &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\#heads&lt;\frac{1}{4}n
&lt;/script&gt;
&lt;/span&gt;. Chernoff’s and Hoeffding’s bounds can be used to &lt;i&gt;bound&lt;/i&gt; the probability for that to happen. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Both bounds are similar in their settings and their results, and sometimes it is not clear which bound is better for a given setting; maybe even both bound are equivalent. In this post we will try to get an intuition which bound is stronger (and when). 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Note that each of the bounds have several common variations and we will discuss only the ones which we considered “popular”. Let’s start with the first bound, a multiplicative version of Chernoff’s bound.
&lt;/div&gt;
&lt;div class=&quot;Definition&quot;&gt;
Chernoff bound. Let &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left\{ X_{i}\right\} _{i=1}^{n}
&lt;/script&gt;
&lt;/span&gt; be independent random variables ranging in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left[0,1\right]
&lt;/script&gt;
&lt;/span&gt;, denote &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
X:=\sum_{i=1}^{n}X_{i}
&lt;/script&gt;
&lt;/span&gt; and let &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mu:=\mathbb{E}\left[X\right]
&lt;/script&gt;
&lt;/span&gt;, then for any &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\epsilon\in\left(0,1\right)
&lt;/script&gt;
&lt;/span&gt; &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{P}\left[\left|X-\mu\right|&gt;\epsilon\mu\right]\le2\exp\left(-\epsilon^{2}\frac{\mu}{3}\right)

&lt;/script&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;Unindented&quot;&gt;

&lt;/div&gt;
&lt;div class=&quot;Definition&quot;&gt;
Hoeffding bound. Let &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left\{ X_{i}\right\} _{i=1}^{n}
&lt;/script&gt;
&lt;/span&gt; be independent random variables ranging in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left[a,b\right]
&lt;/script&gt;
&lt;/span&gt; where &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
a&lt;b
&lt;/script&gt;
&lt;/span&gt;, denote &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
X:=\sum_{i=1}^{n}X_{i}
&lt;/script&gt;
&lt;/span&gt; and let &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mu:=\mathbb{E}\left[X\right]
&lt;/script&gt;
&lt;/span&gt;, then for any &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t
&lt;/script&gt;
&lt;/span&gt;: &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{P}\left[\left|X-\mu\right|&gt;t\right]\le2\exp\left(\frac{-t^{2}}{n\left(b-a\right)^{2}}\right)

&lt;/script&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;Unindented&quot;&gt;

&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-2&quot;&gt;2&lt;/a&gt; Writing Hoeffding’s bound as a Chernoff’s bound 
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
In first glance it seems that we cannot compare between the definitions, mostly because this small technical issue: Hoeffding’s bound allows the random variables to be in any close interval &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left[a,b\right]
&lt;/script&gt;
&lt;/span&gt; rather than &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left[0,1\right]
&lt;/script&gt;
&lt;/span&gt;. The solution for that is to scale and shift the variables to make them takes values in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left[0,1\right]
&lt;/script&gt;
&lt;/span&gt;. We will start with exactly that, and them transform Hoeffding’s inequality into “Chernoff’s”.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
So for any &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
i\in\left[n\right]
&lt;/script&gt;
&lt;/span&gt;, denote &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
Y_{i}=\frac{X_{i}}{b-a}-a
&lt;/script&gt;
&lt;/span&gt; and let &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
Y=\sum_{i=1}^{n}Y_{i}=\frac{X}{b-a}-na
&lt;/script&gt;
&lt;/span&gt;. Observe that &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbb{E}\left[Y\right]=\frac{\mu}{b-a}-na
&lt;/script&gt;
&lt;/span&gt;. Clearly, now the range of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left\{ Y_{i}\right\} _{i=1}^{n}
&lt;/script&gt;
&lt;/span&gt; is &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left[0,1\right]
&lt;/script&gt;
&lt;/span&gt; so we can write Hoeffding’s bound in terms of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
Y_{i}
&lt;/script&gt;
&lt;/span&gt;’s
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
&lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;
\begin{aligned}
\mathbf{P}\left[\left|X-\mu\right|&gt;t\right] &amp; =\mathbf{P}\left[\left|\frac{X}{b-a}-na-\frac{\mu}{b-a}+na\right|&gt;\frac{t}{b-a}\right]\\
 &amp; =\mathbf{P}\left[\left|Y-\mathbb{E}\left[Y\right]\right|&gt;\frac{t}{b-a}\cdot\frac{\mathbb{E}\left[Y\right]}{\mathbb{E}\left[Y\right]}\right]\\
 &amp; \le2\exp\left(\frac{-t^{2}}{\left(b-a\right)^{2}\mathbb{E}\left[Y\right]^{2}}\cdot\frac{\mathbb{E}\left[Y\right]}{3}\right)\\
 &amp; =2\exp\left(\frac{-t^{2}}{3\left(b-a\right)^{2}}\left(\frac{\mu}{b-a}-na\right)^{-1}\right)\\
 &amp; =2\exp\left(\frac{-t^{2}}{3\mu\left(b-a\right)-3a\cdot n\left(b-a\right)^{2}}\right)
\end{aligned}
&lt;/script&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Where the inequality in the middle of the process is where we apply Chernoff’s bound with &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\epsilon=\frac{t}{\left(b-a\right)\mathbb{E}\left[Y\right]}
&lt;/script&gt;
&lt;/span&gt;, note that there is an implicit assumption here that &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\epsilon&lt;1
&lt;/script&gt;
&lt;/span&gt;. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
So we managed to formulate Hoeffding’s bound in Chernoff’s settings, ending with a very messy formula. In order to explore the behavior of the bounds further, we will simplify the analysis by adding another assumption: that &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
a=0
&lt;/script&gt;
&lt;/span&gt;.
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-3&quot;&gt;3&lt;/a&gt; Which bound to used?
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
Now we assuming that &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
a=0
&lt;/script&gt;
&lt;/span&gt;. The restriction from previous section that &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\epsilon&lt;1
&lt;/script&gt;
&lt;/span&gt; is now holds whenever &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t&lt;\mu
&lt;/script&gt;
&lt;/span&gt;, which make sense because Chernoff’s bound, as we defined it in the beginning, only concerned with deviations that are up to &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mu
&lt;/script&gt;
&lt;/span&gt;. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Next, the term we got for Hoeffding is simplified further into &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{P}\left[\left|X-\mu\right|&gt;t\right]\le2\exp\left(\frac{-t^{2}}{3\mu b}\right)

&lt;/script&gt;
&lt;/span&gt;
Now easy to compare between the bounds. With Hoeffding we can achieve bound of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
2\exp\left(\frac{-t^{2}}{nb^{2}}\right)
&lt;/script&gt;
&lt;/span&gt; and with Chernoff &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
2\exp\left(\frac{-t^{2}}{3\mu b}\right)
&lt;/script&gt;
&lt;/span&gt;, it just left to compare between &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
nb
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
3\mu
&lt;/script&gt;
&lt;/span&gt;. If &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
nb&lt;3\mu
&lt;/script&gt;
&lt;/span&gt; then Chernoff is stronger and vise versa. We conclude that none of the bounds is always preferred upon the other (which is not so suprising), and the answer to the question “which one to use” depends on the parameters of the distribution in hand.
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-4&quot;&gt;4&lt;/a&gt; Chernoff bound is tight!
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
One suprising fact about Chernoff’s bound is that in some cases it is tight (i.e. gives a lower bound). The theorem below is taken from those &lt;a class=&quot;URL&quot; href=&quot;https://ece.uwaterloo.ca/~nmousavi/Papers/Chernoff-Tightness.pdf&quot;&gt;notes&lt;/a&gt; (see the link for the full proof).
&lt;/div&gt;
&lt;div class=&quot;Theorem&quot;&gt;
Let &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left\{ X_{i}\right\} _{i=1}^{n}
&lt;/script&gt;
&lt;/span&gt; be i.i.d random variables ranging in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left\{ 0,1\right\} 
&lt;/script&gt;
&lt;/span&gt; with &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathbf{P}\left[X_{i}=1\right]=p
&lt;/script&gt;
&lt;/span&gt;, denote &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
X:=\sum_{i=1}^{n}X_{i}
&lt;/script&gt;
&lt;/span&gt; and let &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mu:=\mathbb{E}\left[X\right]=np
&lt;/script&gt;
&lt;/span&gt;. &lt;br /&gt;
If &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
p\le\frac{1}{4}
&lt;/script&gt;
&lt;/span&gt;, then for any &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t&gt;0
&lt;/script&gt;
&lt;/span&gt; &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{P}\left[X-\mu&gt;t\right]\ge\frac{1}{4}\exp\left(-t^{2}\frac{2}{\mu}\right)

&lt;/script&gt;
&lt;/span&gt;
If &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
p&lt;\frac{1}{2}
&lt;/script&gt;
&lt;/span&gt;, then for any &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
t\in\left[0,n\left(1-2p\right)\right]
&lt;/script&gt;
&lt;/span&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\mathbf{P}\left[X-\mu&gt;t\right]\ge\frac{1}{4}\exp\left(-t^{2}\frac{2}{\mu}\right)

&lt;/script&gt;
&lt;/span&gt;
&lt;br /&gt;&lt;/div&gt;
&lt;div class=&quot;Unindented&quot;&gt;

&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Note the differences between that regime and the one from the definition at the beginning of the post: in this theorem the variables are i.i.d (rather than only independent) and they are discrete.
&lt;/div&gt;</content><author><name>Eran Amar</name></author><category term="probability_theory" /><summary type="html">Warning: MathJax requires JavaScript to correctly process the mathematics on this page. Please enable JavaScript on your browser. &amp;lt;/hr&amp;gt;</summary></entry><entry><title type="html">Introduction to Hypergraphs</title><link href="https://eranamar.github.io/site/2017/03/16/Introduction-to-Hypergraphs.html" rel="alternate" type="text/html" title="Introduction to Hypergraphs" /><published>2017-03-16T00:00:00+02:00</published><updated>2017-03-16T00:00:00+02:00</updated><id>https://eranamar.github.io/site/2017/03/16/Introduction-to-Hypergraphs</id><content type="html" xml:base="https://eranamar.github.io/site/2017/03/16/Introduction-to-Hypergraphs.html">&lt;script type=&quot;math/tex&quot;&gt;
\newcommand{\lyxlock}{}
&lt;/script&gt;

&lt;noscript&gt;
&lt;div class=&quot;warning&quot;&gt;
Warning: &lt;a href=&quot;http://www.mathjax.org/&quot;&gt;MathJax&lt;/a&gt; requires JavaScript to correctly process the mathematics on this page. Please enable JavaScript on your browser.
&lt;/div&gt;&lt;hr /&gt;
&amp;lt;/hr&amp;gt;&lt;/noscript&gt;

&lt;div class=&quot;Unindented&quot;&gt;
In this post, I will review some of the basic definitions about hypergraphs and cuts sparsifier on them. I will start from the very basic definitions about Graphs, then generalizing them to Hypergraphs. Later, I will present the definition of cuts sparsifier and some lower bound related to them.
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-1&quot;&gt;1&lt;/a&gt; Basic definitions in Graphs
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
Let’s start from the very beginning - graphs. Graphs are fundamental objects in computer science, and they are used to model relations between objects. Formally, a &lt;i&gt;graph&lt;/i&gt; &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
G=\left(V,E,w\right)
&lt;/script&gt;
&lt;/span&gt; is a tuple of two sets, &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
V
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
E
&lt;/script&gt;
&lt;/span&gt; and a function &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
w
&lt;/script&gt;
&lt;/span&gt;. &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
V
&lt;/script&gt;
&lt;/span&gt; is the set of &lt;i&gt;vertices&lt;/i&gt; (i.e. the elements), usually denoted as &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left[n\right]:=\left\{ 1,2,..,n\right\} 
&lt;/script&gt;
&lt;/span&gt;, and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
E
&lt;/script&gt;
&lt;/span&gt; is a set of &lt;i&gt;edges&lt;/i&gt; between those vertices (i.e. indicating which elements belongs to the relation the graph represents). An edge is just a set of two vertices, that is &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
e=\left\{ u,v\right\} \in E\subseteq\left\{ A\mid A\subseteq V,\:\:\left|A\right|=2\right\} 
&lt;/script&gt;
&lt;/span&gt;. The function &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
w:\:E\rightarrow\mathbb{R}_{\ge0}
&lt;/script&gt;
&lt;/span&gt; assign non-negative weight for each edge in the graph, and called weight function. For instance, we can model friendships in Facebook as a graph, the vertices will be all the users in Facebook, and there will be an edge between any pair of users if they are “friends” of each other.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
A &lt;i&gt;cut&lt;/i&gt; in a graph &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
G
&lt;/script&gt;
&lt;/span&gt; is a partition of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
V
&lt;/script&gt;
&lt;/span&gt; into two sets, &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left(S,V\backslash S\right)
&lt;/script&gt;
&lt;/span&gt;. We say that an edge &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
e=\left\{ u,v\right\} 
&lt;/script&gt;
&lt;/span&gt; &lt;i&gt;cross&lt;/i&gt; the cut defined by &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
S
&lt;/script&gt;
&lt;/span&gt; if and only if &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
S\cap e\ne\emptyset
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left(V\backslash S\right)\cap e\ne\emptyset
&lt;/script&gt;
&lt;/span&gt;, that is, the edge “touches” both parts of the graph. Now we can talk about the &lt;i&gt;weight of the cut&lt;/i&gt;, that is, if we have a cut that is defined by &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
S
&lt;/script&gt;
&lt;/span&gt; and a weight function &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
w
&lt;/script&gt;
&lt;/span&gt; then, &lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

w_{G}\left(S\right)=\sum_{\begin{array}{c}
e\in E\\
e\cap S\notin\{\emptyset,e\}
\end{array}}w(e)

&lt;/script&gt;
&lt;/span&gt;
in words, the weight of a cut is the sum of weights of all edges that cross it.
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-2&quot;&gt;2&lt;/a&gt; Generalizing to Hyper-Graphs
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
The model of a graph, as defined above, can’t be used to describe non binary relationships. For example, say we want to model Facebook groups via a graph; the only way to do so is by including in the set of vertices also all the Facebook groups and then connecting each user to all its groups with edges. The main limitation here is that the vertices have more than one meaning (they can describe both &lt;i&gt;users&lt;/i&gt; and &lt;i&gt;groups&lt;/i&gt;). In order to avoid such ambiguity we can use a richer model, we can use Hypergraphs.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Hypergraphs are generalization of graphs in the sense that edges may be of arbitrary size. Meaning that now &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
E\subseteq2^{V}\backslash\emptyset=\left\{ A\mid A\subseteq V,\:\:\left|A\right|&gt;0\right\} 
&lt;/script&gt;
&lt;/span&gt;. Going back to our example, we can model Facebook groups by the graph &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left(V,E\right)
&lt;/script&gt;
&lt;/span&gt; when &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
V
&lt;/script&gt;
&lt;/span&gt; is the set of all users, and any group in Facebook will be an edge &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
e\in2^{V}
&lt;/script&gt;
&lt;/span&gt; such that &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
e
&lt;/script&gt;
&lt;/span&gt; contains the users belongs to that group. More mathematical example will be &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
G=\left(\left[5\right],\left\{ \left\{ 4\right\} ,\left\{ 1,2,5\right\} \right\} \right)
&lt;/script&gt;
&lt;/span&gt;, which is a valid hypergraph with two edges: &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left\{ 4\right\} 
&lt;/script&gt;
&lt;/span&gt; and &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left\{ 1,2,5\right\} 
&lt;/script&gt;
&lt;/span&gt;. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Note that the definition of &lt;i&gt;cuts&lt;/i&gt; and &lt;i&gt;cut weight&lt;/i&gt; can be transfer easily into hypergraphs because they stay the same as with regular graphs. Usually we will be interested in family of hypergraphs with limited size, that is, hypergraphs where each edge is of size at most &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
r
&lt;/script&gt;
&lt;/span&gt;. Such hypergraphs are said to be &lt;i&gt;r-uniform&lt;/i&gt;.
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-3&quot;&gt;3&lt;/a&gt; Cuts Sparsification 
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
Let &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\epsilon\in\left(0,1\right)
&lt;/script&gt;
&lt;/span&gt;. Given a hypergraph &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
H=\left(V,E\right)
&lt;/script&gt;
&lt;/span&gt; and a weight function &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
w
&lt;/script&gt;
&lt;/span&gt;, we say that a hypergraph &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
K=\left(V,E_{\epsilon},w\right)
&lt;/script&gt;
&lt;/span&gt; is &lt;i&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\epsilon
&lt;/script&gt;
&lt;/span&gt;-cut-sparsifier&lt;/i&gt; of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
H
&lt;/script&gt;
&lt;/span&gt; if &lt;i&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;
&lt;script type=&quot;math/tex;mode=display&quot;&gt;

\forall S\subset V\qquad(1-\epsilon)\cdot w_{H}(S)\le w_{K}(S)\le(1+\epsilon)\cdot w_{H}(S)

&lt;/script&gt;
&lt;/span&gt;
&lt;/i&gt;and the set &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
E_{\epsilon}
&lt;/script&gt;
&lt;/span&gt; may be any set in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
2^{V}\backslash\emptyset
&lt;/script&gt;
&lt;/span&gt;.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
It is not part of the definition, but the goal is to find cut-sparsifier that shrink the number of edges, that is, with the smallest &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\left|E_{\epsilon}\right|
&lt;/script&gt;
&lt;/span&gt; possible. Whereas in regular graphs it is understood that we want to minimize the &lt;b&gt;number&lt;/b&gt; of edges, in hypergraphs we should also consider the &lt;b&gt;size&lt;/b&gt; of the edges in &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
E_{\epsilon}
&lt;/script&gt;
&lt;/span&gt;. That is, the quality of the sparsifier will be measure also with respect to the size of the edges in the resulting graph. For that we introduce the &lt;i&gt;total edge size&lt;/i&gt; of a hypergraph which is &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\sum_{e\in E_{\epsilon}}\left|e\right|
&lt;/script&gt;
&lt;/span&gt;.
&lt;/div&gt;
&lt;h1 class=&quot;Section&quot;&gt;
&lt;a class=&quot;toc&quot; name=&quot;toc-Section-4&quot;&gt;4&lt;/a&gt; Upper &amp;amp; Lower Bounds for Sparsification
&lt;/h1&gt;
&lt;div class=&quot;Unindented&quot;&gt;
For regular graphs, we&lt;a class=&quot;URL&quot; href=&quot;http://dx.doi.org/10.1016/j.jpdc.2009.04.011&quot;&gt; already know&lt;/a&gt; how to reduce the number of edges to be &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{O}\left(n/\epsilon^{2}\right)
&lt;/script&gt;
&lt;/span&gt; for any &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\epsilon\in\left(0,1\right)
&lt;/script&gt;
&lt;/span&gt;. That is surprising, because no matter how many edges there are in the original graph, which can be up to &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
n^{2}
&lt;/script&gt;
&lt;/span&gt;, there is an algorithms that can reduce the number of edges to &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{O}\left(n/\epsilon^{2}\right)
&lt;/script&gt;
&lt;/span&gt;, while maintaining approximately the same weights for all possible cuts in the graph. Moreover, that algorithm find the cut-sparsifier in polynomial time. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
Later results show that this upper bound is also a lower bound, that is, there are graphs that cannot be reduced into less that &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
\mathcal{O}\left(n/\epsilon^{2}\right)
&lt;/script&gt;
&lt;/span&gt; edges.
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
However, when working with hypergraphs it is not quite clear if one can reduce the total edges size of the graph to be even order of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
n^{2}
&lt;/script&gt;
&lt;/span&gt;. Think about it, potentially the number of edges in a hypergraph can be &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
2^{n}
&lt;/script&gt;
&lt;/span&gt;, so &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
n^{2}
&lt;/script&gt;
&lt;/span&gt; can be considered quite small for such hypergraphs. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
An interesting&lt;a class=&quot;URL&quot; href=&quot;https://arxiv.org/abs/1409.2391&quot;&gt; result from 2015&lt;/a&gt;, showed lower bound of &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
O(\epsilon^{-2}n\cdot r)
&lt;/script&gt;
&lt;/span&gt; edges for &lt;i&gt;r-&lt;/i&gt;uniform hypergraphs (assuming that &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
r&gt;\log\left(n\right)
&lt;/script&gt;
&lt;/span&gt;), which, when converting to total edges size is actually &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;
O(\epsilon^{-2}n\cdot r^{2})
&lt;/script&gt;
&lt;/span&gt;. 
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
The question that one can ask now is, can we do better? Is there an algorithm that can construct a cut-sparsifier for hypergraphs with &lt;i&gt;smaller&lt;/i&gt; total edges size?
&lt;/div&gt;
&lt;div class=&quot;Indented&quot;&gt;
That is, actually, an open question which is studied nowdays.
&lt;/div&gt;</content><author><name>Eran Amar</name></author><category term="hypergraphs" /><category term="cuts_sparsifier" /><summary type="html">Warning: MathJax requires JavaScript to correctly process the mathematics on this page. Please enable JavaScript on your browser. &amp;lt;/hr&amp;gt;</summary></entry></feed>